<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="description" content="bioregion">
<title>4.1 Hierarchical clustering • bioregion</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.2.2/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.2.2/bootstrap.bundle.min.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- bootstrap-toc --><script src="https://cdn.jsdelivr.net/gh/afeld/bootstrap-toc@v1.0.1/dist/bootstrap-toc.min.js" integrity="sha256-4veVQbu7//Lk5TSmc7YV48MxtMy98e26cf5MrgZYnwo=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- search --><script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/6.4.6/fuse.js" integrity="sha512-zv6Ywkjyktsohkbp9bb45V6tEMoWhzFzXis+LrMehmJZZSys19Yxf1dopHx7WzIKxr5tK2dVcYmaCk2uqdjF4A==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/autocomplete.js/0.38.0/autocomplete.jquery.min.js" integrity="sha512-GU9ayf+66Xx2TmpxqJpliWbT5PiGYxpaG8rfnBEk1LL8l1KGkRShhngwdXK1UgqhAzWpZHSiYPc09/NwDQIGyg==" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/mark.min.js" integrity="sha512-5CYOlHXGh6QpOFA/TeTylKLWfB3ftPsde7AnmhuitiTX4K5SqCLBeKro6sPS8ilsz1Q4NRx3v8Ko2IBiszzdww==" crossorigin="anonymous"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="4.1 Hierarchical clustering">
<meta property="og:description" content="bioregion">
<meta property="og:image" content="https://bioRgeo.github.io/bioregion/logo.svg">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>
    

    <nav class="navbar fixed-top navbar-light navbar-expand-lg bg-light"><div class="container">
    
    <a class="navbar-brand me-2" href="../index.html">bioregion</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.0.0</small>

    
    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item">
  <a class="nav-link" href="../articles/bioregion.html">Get started</a>
</li>
<li class="nav-item">
  <a class="nav-link" href="../reference/index.html">Reference</a>
</li>
<li class="active nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-articles">Articles</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-articles">
    <a class="dropdown-item" href="../articles/a1_install_binary_files.html">1. Installation of the binary files</a>
    <a class="dropdown-item" href="../articles/a2_matrix_and_network_formats.html">2. Matrix and network formats</a>
    <a class="dropdown-item" href="../articles/a3_pairwise_metrics.html">3. Pairwise similarity/dissimilarity metrics</a>
    <a class="dropdown-item" href="../articles/a4_1_hierarchical_clustering.html">4.1 Hierarchical clustering</a>
    <a class="dropdown-item" href="../articles/a4_2_non_hierarchical_clustering.html">4.2 Non-hierarchical clustering</a>
    <a class="dropdown-item" href="../articles/a4_3_network_clustering.html">4.3 Network clustering</a>
    <a class="dropdown-item" href="../articles/a4_4_microbenchmark.html">4.4 Microbenchmark</a>
    <a class="dropdown-item" href="../articles/a5_visualization.html">5. Visualization</a>
  </div>
</li>
<li class="nav-item dropdown">
  <a href="#" class="nav-link dropdown-toggle" data-bs-toggle="dropdown" role="button" aria-expanded="false" aria-haspopup="true" id="dropdown-news">News</a>
  <div class="dropdown-menu" aria-labelledby="dropdown-news">
    <h6 class="dropdown-header" data-toc-skip>Releases</h6>
    <a class="external-link dropdown-item" href="https://github.com/bioRgeo/bioregion/releases/tag/v1.0.0">Version 1.0.0</a>
    <div class="dropdown-divider"></div>
    <a class="dropdown-item" href="../news/index.html">Changelog</a>
  </div>
</li>
      </ul>
<form class="form-inline my-2 my-lg-0" role="search">
        <input type="search" class="form-control me-sm-2" aria-label="Toggle navigation" name="search-input" data-search-index="../search.json" id="search-input" placeholder="Search for" autocomplete="off">
</form>

      <ul class="navbar-nav">
<li class="nav-item">
  <a class="external-link nav-link" href="https://github.com/bioRgeo/bioregion/" aria-label="github">
    <span class="fab fa fab fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>

    
  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>4.1 Hierarchical clustering</h1>
                        <h4 data-toc-skip class="author">Boris Leroy,
Maxime Lenormand and Pierre Denelle</h4>
            
            <h4 data-toc-skip class="date">2023-06-13</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/bioRgeo/bioregion/blob/HEAD/vignettes/a4_1_hierarchical_clustering.Rmd" class="external-link"><code>vignettes/a4_1_hierarchical_clustering.Rmd</code></a></small>
      <div class="d-none name"><code>a4_1_hierarchical_clustering.Rmd</code></div>
    </div>

    
    
<p>Hierarchical clustering consists in creating a hierarchical tree from
a matrix of distances (or beta-diversities). From this hierarchical
tree, clusters can be obtained by cutting the tree.</p>
<p>Although these methods are conceptually simple, their implementation
can be complex and requires important choices by the user. In the
following, we provide a step-by-step guide on how to do hierarchical
clustering analyses with bioregion, along with comments on the
philosophy on how we designed the functions.</p>
<p>Hierarchical clustering takes place on the right-hand size part of
the bioregion conceptual diagram:</p>
<p><img src="figures/diagram_nonetwork.png"></p>
<div class="section level2">
<h2 id="compute-dissimilarity-indices-from-input-data">1. Compute dissimilarity indices from input data<a class="anchor" aria-label="anchor" href="#compute-dissimilarity-indices-from-input-data"></a>
</h2>
<p>To initiate the hierarchical clustering procedure, you need to
provide pairwise distances between sites. These pairwise distances
between sites can be obtained by running <code><a href="../reference/dissimilarity.html">dissimilarity()</a></code> on
a species-site matrix, such as a presence-absence or an abundance
matrix.</p>
<p>In the example below, we use the vegetation dataset from the package
to compute distance metrics.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/bioRgeo/bioregion" class="external-link">bioregion</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Work with the vegetation dataset we include in the package</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># This is an abundance matrix where sites are in rows and species in columns</span></span>
<span><span class="va">vegemat</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span></code></pre></div>
<pre><code><span><span class="co">##     Species</span></span>
<span><span class="co">## Site 10001 10002 10003 10004 10005 10006 10007 10008 10009 10010</span></span>
<span><span class="co">##   35     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   36     2     0     0     0     0     0     1    12     0     0</span></span>
<span><span class="co">##   37     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   38     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   39     5     0     0     0     0     0     0     2     0     0</span></span>
<span><span class="co">##   84     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   85     3     0     0     0     0     0     1     7     0     0</span></span>
<span><span class="co">##   86     0     0     0     2     0     0     2    22     0     0</span></span>
<span><span class="co">##   87    16     0     0     0     0     0     2    54     0     0</span></span>
<span><span class="co">##   88   228     0     0     0     0     0     0     5     0     0</span></span></code></pre>
<p>We are going to compute the <span class="math inline">\(\beta_{sim}\)</span> diversity metric, which is a
presence-absence dissimilarity index. The formula is as follows: <span class="math inline">\(\beta_{sim} = min(b, c) / (a+min(b,
c))\)</span></p>
<p>Where <em>a</em> is the number of species shared by both sites;
<em>b</em> is the number of species occurring only in the first site;
and <em>c</em> is the number of species only occurring only in the
second site.</p>
<p>We typically choose this metric for bioregionalisation, because it is
the <strong>turnover</strong> component of the Sorensen index <span class="citation">(Baselga, 2012)</span> (in a nutshell, it tells us how
sites are different because they have distinct species), and because it
has less dependence on species richness than the Jaccard turnover <span class="citation">(Leprieur &amp; Oikonomou, 2014)</span>. Alternatively,
given that we have abundance data here, we could also use the
Bray-Curtis turnover index <span class="citation">(Baselga,
2013)</span>. The choice of the distance metric is very important for
the outcome of the clustering procedure, so we recommend that you choose
carefully depending on your research question.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dissim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dissimilarity.html">dissimilarity</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Data.frame of dissimilarity between sites</span></span>
<span><span class="co">##  - Total number of sites:  7 </span></span>
<span><span class="co">##  - Number of rows:  6 </span></span>
<span><span class="co">##  - Number of dissimilarity metrics:  1 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">##   Site1 Site2    Simpson</span></span>
<span><span class="co">## 2    35    36 0.02325581</span></span>
<span><span class="co">## 3    35    37 0.03100775</span></span>
<span><span class="co">## 4    35    38 0.05426357</span></span>
<span><span class="co">## 5    35    39 0.05426357</span></span>
<span><span class="co">## 6    35    84 0.72093023</span></span>
<span><span class="co">## 7    35    85 0.08527132</span></span></code></pre>
<p>By default, only the Simpson index is computed, but other options are
available in the <code>metric</code> argument of dissimilarity().
Furthermore, users can also write down their own formula to compute any
index they wish for in the argument <code>formula</code>, see
?dissimilarity().</p>
<p>We are now ready to start the hierarchical clustering procedure with
the object <code>dissim</code> we have just created. Alternatively, you
can also use other types of objects in hclu_hierarclust(), such as a
distance matrix object (class <code>dist</code>) or a
<code>data.frame</code> of your own crafting (make sure to read the
required format carefully in ?hclu_hierarclust).</p>
</div>
<div class="section level2">
<h2 id="hierarchical-clustering-with-basic-parameters">2. Hierarchical clustering with basic parameters<a class="anchor" aria-label="anchor" href="#hierarchical-clustering-with-basic-parameters"></a>
</h2>
<div class="section level3">
<h3 id="basic-usage">2.1 Basic usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h3>
<p>The basic usage of the function is as follows:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Randomizing the dissimilarity matrix with 30 trials</span></span></code></pre>
<pre><code><span><span class="co">##  -- range of cophenetic correlation coefficients among</span></span>
<span><span class="co">##                      trials: 0.65 - 0.71</span></span></code></pre>
<pre><code><span><span class="co">## Optimal tree has a 0.71 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<p>The functions gives us some information as it proceeds. Notably, it
talks about a randomization of the dissimilarity matrix - this is a very
important feature because hierarchical clustering is heavily influenced
by the order of sites in the distance matrix. Therefore, by default, the
function performs a randomization of the order of sites in the distance
matrix with 30 trials (<a href="#randomization">more information in the
randomization section</a>). It also tells us that among all trials it
has selected the tree with the highest cophenetic correlation
coefficient, at a value of 0.71.</p>
<p>We can see type the name of the object in the console to see more
information:</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hierarchical_clustering </span></span>
<span><span class="co">##  (hierarchical clustering based on a dissimilarity matrix)</span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">##  - Name of dissimilarity metric:  Simpson </span></span>
<span><span class="co">##  - Tree construction method:  average </span></span>
<span><span class="co">##  - Randomization of the dissimilarity matrix:  yes, number of trials 30 </span></span>
<span><span class="co">##  - Cophenetic correlation coefficient:  0.712 </span></span>
<span><span class="co">## Clustering procedure incomplete - no clusters yet</span></span></code></pre>
<p>The last line tells us that the the clustering procedure is
incomplete: the tree has been built, but it has not been cut yet - hence
there are no clusters yet in the object.</p>
<p>To cut the tree, we can use the function cut_tree():</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ask for 4 clusters</span></span>
<span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree1</span>,</span>
<span>                  n_clust <span class="op">=</span> <span class="fl">4</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Determining the cut height to reach 4 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.53515625</span></span></code></pre>
<p>Here, we asked for 4 clusters, and the algorithm automatically finds
the height at which 4 clusters are found (h = 0.535).</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hierarchical_clustering </span></span>
<span><span class="co">##  (hierarchical clustering based on a dissimilarity matrix)</span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">##  - Name of dissimilarity metric:  Simpson </span></span>
<span><span class="co">##  - Tree construction method:  average </span></span>
<span><span class="co">##  - Randomization of the dissimilarity matrix:  yes, number of trials 30 </span></span>
<span><span class="co">##  - Cophenetic correlation coefficient:  0.712 </span></span>
<span><span class="co">##  - Number of clusters requested by the user:  4 </span></span>
<span><span class="co">## Clustering results:</span></span>
<span><span class="co">##  - Number of partitions:  1 </span></span>
<span><span class="co">##  - Number of clusters:  4 </span></span>
<span><span class="co">##  - Height of cut of the hierarchical tree: 0.535</span></span></code></pre>
<p>When we type again the name of the object in the console, it gives us
the results of the clustering: we have</p>
<ul>
<li>
<strong>1 partition</strong>: a partition is a clustering result. We
only cut the tree once, so we only have 1 partition at the moment</li>
<li>
<strong>4 clusters</strong>: this is the number of clusters in the
partition. We asked for 4, and we obtained 4, which is good. Sometimes,
however, we cannot get the number of clusters we asked for - in which
case the outcome will be indicated.</li>
<li>**a heigh of cut at 0.535: this is the height of cut at which we can
obtain 4 clusters in our tree.</li>
</ul>
<p>We can make a quick plot of our partitioned tree with</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># We reduced the size of text labels with cex = .2, because there are too many sites</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree1</span>, cex <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-7-1.png" width="1152"></p>
<p>Now, this is a hierarchical tree, and cutting it only once (= only 1
partition) oversimplifies the result of the tree. Why not cut it
multiple times? For example, we could make deep, intermediate, and
shallow cuts to the tree, likewise to <span class="citation">Ficetola et
al. (2017)</span>, which would allow us to see broad- to fine-scale
relationships among sites in our tree.</p>
<p>We can specify, e.g. 4, 10 and 20 clusters:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ask for 4, 10 and 20 clusters</span></span>
<span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree1</span>,</span>
<span>                  n_clust <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">4</span>, <span class="fl">10</span>, <span class="fl">20</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Determining the cut height to reach 4 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.53515625</span></span></code></pre>
<pre><code><span><span class="co">## Determining the cut height to reach 10 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.46240234375</span></span></code></pre>
<pre><code><span><span class="co">## Determining the cut height to reach 20 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.40625</span></span></code></pre>
<div class="sourceCode" id="cb24"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree1</span>, cex <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-8-1.png" width="1152">
However, it may be more useful to choose the heights of cut, rather than
the number of clusters. We could, for example, cut the tree at heights
0.4 (shallow cut), 0.5 (intermediate cut) and 0.6 (deep cut):</p>
<div class="sourceCode" id="cb25"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree1</span>,</span>
<span>                  cut_height <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">.5</span>, <span class="fl">.6</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree1</span>, cex <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-9-1.png" width="1152"></p>
<p>The plot is not easy to read because of the large number of sites. We
can rather extract the information directly from the object:</p>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hierarchical_clustering </span></span>
<span><span class="co">##  (hierarchical clustering based on a dissimilarity matrix)</span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">##  - Name of dissimilarity metric:  Simpson </span></span>
<span><span class="co">##  - Tree construction method:  average </span></span>
<span><span class="co">##  - Randomization of the dissimilarity matrix:  yes, number of trials 30 </span></span>
<span><span class="co">##  - Cophenetic correlation coefficient:  0.712 </span></span>
<span><span class="co">##  - Heights of cut requested by the user:  0.4 0.5 0.6 </span></span>
<span><span class="co">## Clustering results:</span></span>
<span><span class="co">##  - Number of partitions:  3 </span></span>
<span><span class="co">##  - Partitions are not hierarchical</span></span>
<span><span class="co">##  - Number of clusters:  2 8 21 </span></span>
<span><span class="co">##  - Height of cut of the hierarchical tree: 0.6 0.5 0.4</span></span></code></pre>
<p>From the result, we can read that for the deep cut partition (h =
0.6) we have clusters, for the intermediate cut partition (h = 0.5) we
have 8 clusters and for the shallow cut partition (h = 0.4) we have 21
clusters.</p>
<p>In the next section we will see what are the default settings and why
we chose them, and then we will see how to find optimal numbers of
clusters.</p>
</div>
<div class="section level3">
<h3 id="exploring-the-outputs">2.2 Exploring the outputs<a class="anchor" aria-label="anchor" href="#exploring-the-outputs"></a>
</h3>
<p>To explore the object, you can use <code><a href="https://rdrr.io/r/utils/str.html" class="external-link">str()</a></code> to see the
object structure:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">tree1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 6</span></span>
<span><span class="co">##  $ name        : chr "hierarchical_clustering"</span></span>
<span><span class="co">##  $ args        :List of 11</span></span>
<span><span class="co">##   ..$ index              : chr "Simpson"</span></span>
<span><span class="co">##   ..$ method             : chr "average"</span></span>
<span><span class="co">##   ..$ randomize          : logi TRUE</span></span>
<span><span class="co">##   ..$ n_runs             : num 30</span></span>
<span><span class="co">##   ..$ optimal_tree_method: chr "best"</span></span>
<span><span class="co">##   ..$ n_clust            : NULL</span></span>
<span><span class="co">##   ..$ cut_height         : num [1:3] 0.4 0.5 0.6</span></span>
<span><span class="co">##   ..$ find_h             : logi TRUE</span></span>
<span><span class="co">##   ..$ h_max              : num 1</span></span>
<span><span class="co">##   ..$ h_min              : num 0</span></span>
<span><span class="co">##   ..$ dynamic_tree_cut   : logi FALSE</span></span>
<span><span class="co">##  $ inputs      :List of 6</span></span>
<span><span class="co">##   ..$ bipartite      : logi FALSE</span></span>
<span><span class="co">##   ..$ weight         : logi TRUE</span></span>
<span><span class="co">##   ..$ pairwise_metric: logi TRUE</span></span>
<span><span class="co">##   ..$ dissimilarity  : logi TRUE</span></span>
<span><span class="co">##   ..$ nb_sites       : int 715</span></span>
<span><span class="co">##   ..$ hierarchical   : logi FALSE</span></span>
<span><span class="co">##  $ algorithm   :List of 5</span></span>
<span><span class="co">##   ..$ trials             : chr "Trials not stored in output"</span></span>
<span><span class="co">##   ..$ final.tree         :List of 7</span></span>
<span><span class="co">##   .. ..- attr(*, "class")= chr "hclust"</span></span>
<span><span class="co">##   ..$ final.tree.coph.cor: num 0.712</span></span>
<span><span class="co">##   ..$ output_n_clust     : Named int [1:3] 2 8 21</span></span>
<span><span class="co">##   .. ..- attr(*, "names")= chr [1:3] "h_0.6" "h_0.5" "h_0.4"</span></span>
<span><span class="co">##   ..$ output_cut_height  : num [1:3] 0.6 0.5 0.4</span></span>
<span><span class="co">##  $ clusters    :'data.frame':    715 obs. of  4 variables:</span></span>
<span><span class="co">##   ..$ ID  : chr [1:715] "602" "610" "558" "766" ...</span></span>
<span><span class="co">##   ..$ K_2 : chr [1:715] "1" "2" "2" "2" ...</span></span>
<span><span class="co">##   ..$ K_8 : chr [1:715] "1" "2" "2" "2" ...</span></span>
<span><span class="co">##   ..$ K_21: chr [1:715] "1" "2" "2" "2" ...</span></span>
<span><span class="co">##  $ cluster_info:'data.frame':    3 obs. of  3 variables:</span></span>
<span><span class="co">##   ..$ partition_name      : chr [1:3] "K_2" "K_8" "K_21"</span></span>
<span><span class="co">##   ..$ n_clust             : int [1:3] 2 8 21</span></span>
<span><span class="co">##   ..$ requested_cut_height: num [1:3] 0.6 0.5 0.4</span></span>
<span><span class="co">##  - attr(*, "class")= chr [1:2] "bioregion.clusters" "list"</span></span></code></pre>
<p>It show you the different slots in the object, and how you can access
them. For example, if I want to access the <code>clusters</code> slot, I
have to type <code>tree1$clusters</code>.</p>
<ul>
<li>
<strong>name</strong>: the name of the method we are using</li>
<li>
<strong>args</strong>: the arguments you have selected for your
tree</li>
<li>
<strong>inputs</strong>: this is mostly for internal use in the
package, it provides some info about the nature of input data and
methods</li>
<li>
<strong>algorithm</strong>: this slot contains detailed information
about the hierarchical clustering. For example, you can have access to
the raw tree here, in <code>hclust</code> format. To access it, I can
type <code>tree1$algorithm$final.tree</code>
</li>
<li>
<strong>clusters</strong>: this is a <code>data.frame</code>
containing your partitions. The first column is your sites, and all the
other columns are the partitions.</li>
<li>
<strong>cluster_info</strong>: this is a small
<code>data.frame</code> which will help you link your requests with the
<code>clusters</code> <code>data.frame</code>. Its content varies
depending on your choices; for example, in my case, it looks like
this:</li>
</ul>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span><span class="op">$</span><span class="va">cluster_info</span></span></code></pre></div>
<pre><code><span><span class="co">##       partition_name n_clust requested_cut_height</span></span>
<span><span class="co">## h_0.6            K_2       2                  0.6</span></span>
<span><span class="co">## h_0.5            K_8       8                  0.5</span></span>
<span><span class="co">## h_0.4           K_21      21                  0.4</span></span></code></pre>
<p>It shows the name of the partition (corresponding to column names in
<code>tree1$clusters</code>), the number of clusters in each partition,
and the cut height I initially requested.</p>
</div>
<div class="section level3">
<h3 id="explanation-of-the-default-settings-and-how-to-change-them">2.3 Explanation of the default settings and how to change them<a class="anchor" aria-label="anchor" href="#explanation-of-the-default-settings-and-how-to-change-them"></a>
</h3>
<div class="section level4">
<h4 id="randomization">2.3.1 Randomization of the distance matrix<a class="anchor" aria-label="anchor" href="#randomization"></a>
</h4>
<p>The order of sites in the distance matrix influences the outcome of
the hierarchical tree. Let’s see that with an example:</p>
<div class="sourceCode" id="cb32"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute the tree without randomizing the distance matrix</span></span>
<span><span class="va">tree2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim</span>,</span>
<span>                          randomize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Output tree has a 0.7 cophenetic correlation coefficient with the initial</span></span>
<span><span class="co">##                    dissimilarity matrix</span></span></code></pre>
<div class="sourceCode" id="cb34"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree2</span>, cex <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-13-1.png" width="768"></p>
<p>This is how the tree looks like when the matrix is not randomized.
Now let’s randomize it and regenerate the tree:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># This line randomizes the order of rows in the distance matrix</span></span>
<span><span class="va">dissim_random</span> <span class="op">&lt;-</span> <span class="va">dissim</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># Recompute the tree</span></span>
<span><span class="va">tree3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim_random</span>,</span>
<span>                          randomize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Output tree has a 0.7 cophenetic correlation coefficient with the initial</span></span>
<span><span class="co">##                    dissimilarity matrix</span></span></code></pre>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree3</span>, cex <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-14-1.png" width="768"></p>
<p>See how the tree looks different? This is problematic because it
means that the outcome is heavily influenced by the order of sites in
the distance matrix.</p>
<p>To address this issue, we randomize the distance matrix multiple
times (30, by default, but you can probably increase the number to 100
or more) and generate all the associated trees. For each individual
tree, the function calculates the <strong>cophenetic correlation
coefficient</strong>, which is the <em>correlation between the initial
distance <span class="math inline">\(\beta_{sim}\)</span> among
sites</em> and <em>the cophenetic distance</em>, which is the distance
at which sites are connected in the tree. It tells us how representative
is the tree of the initial distance matrix.</p>
<p>The next question is what to do with all the individual trees? By
default, in the package we select the tree that best represents the
distance matrix; i.e., the one that has the highest cophenetic
correlation coefficient (argument
<code>optimal_tree_method = "best"</code>)</p>
<p>Let’s see an example with a higher number of runs
(<code>n_runs = 100</code>). We can also ask the function to keep all
individual trees for further exploration
(<code>keep_trials = TRUE</code>).</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim_random</span>,</span>
<span>                          randomize <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                          n_runs <span class="op">=</span> <span class="fl">100</span>,</span>
<span>                          keep_trials <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Randomizing the dissimilarity matrix with 100 trials</span></span></code></pre>
<pre><code><span><span class="co">##  -- range of cophenetic correlation coefficients among</span></span>
<span><span class="co">##                      trials: 0.65 - 0.71</span></span></code></pre>
<pre><code><span><span class="co">## Optimal tree has a 0.71 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<p>Another approach could be to build a consensus tree from all the
individual trees, and use this tree, as is done for example in <a href="https://github.com/leondap/recluster" class="external-link">the package recluster</a>,
but this is not yet available in bioregion.</p>
</div>
<div class="section level4">
<h4 id="tree-construction-algorithm">2.3.2 Tree construction algorithm<a class="anchor" aria-label="anchor" href="#tree-construction-algorithm"></a>
</h4>
<p>By default, the function uses the UPGMA method (Unweighted Pair Group
Method with Arithmetic Mean) because it has been recommended in
bioregionalisation for its better performance over other approaches
<span class="citation">(Kreft &amp; Jetz, 2010)</span>. You can change
this method by changing the argument <code>method</code>; all methods
implemented in <code><a href="https://rdrr.io/r/stats/hclust.html" class="external-link">stats::hclust()</a></code> are available.</p>
</div>
<div class="section level4">
<h4 id="cutting-the-tree">2.3.3 Cutting the tree<a class="anchor" aria-label="anchor" href="#cutting-the-tree"></a>
</h4>
<p>There are three ways of cutting the tree:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Specify the expected number of clusters</strong>: you can
request a specific number of clusters (<code>n_clust = 5</code> for
example). You can also request multiple partitions, each with their own
number of clusters (<code>n_clust = c(5, 10, 15)</code>) for
example.</li>
</ol>
<p><strong>Note:</strong> When you specify the number of clusters, the
function will search for the associated height of cut automatically; you
can disable this parameter with <code>find_h = FALSE</code>. It will
search for this <em>h</em> value between <code>h_max</code> (default 1)
and <code>h_min</code> (default 0). These arguments can be adjusted if
you are working with indices whose values do not range between 0 and
1.</p>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>Specify the height of cut</strong>: you can request the
height at which you want to cut the tree (e.g.,
<code>cut_height = 0.5</code>). You can also request multiple
partitions, each with their own cut height
(<code>cut_height = c(0.4, 0.5, 0.6)</code>) for example.</p></li>
<li><p><strong>Use a dynamic tree cut method</strong>: Rather than
cutting the entire tree at once, this alternative approach consists in
cutting individual branches at different heights. This method can be
requested by using <code>dynamic_tree_cut = TRUE</code>, and is based on
the dynamicTreeCut R package.</p></li>
</ol>
</div>
</div>
</div>
<div class="section level2">
<h2 id="optimaln">3. How to find an optimal number of clusters?<a class="anchor" aria-label="anchor" href="#optimaln"></a>
</h2>
<p><img src="figures/find_optimal_n.png"></p>
<ol style="list-style-type: decimal">
<li><p>Step 1. <strong>Build a tree</strong> with
<code><a href="../reference/hclu_hierarclust.html">hclu_hierarclust()</a></code></p></li>
<li><p>Step 2. <strong>Explore a range of partitions</strong>, from a
minimum (e.g., starting at 2 clusters) up to a maximum (e.g. <span class="math inline">\(n-1\)</span> clusters where <span class="math inline">\(n\)</span> is the number of sites).</p></li>
<li><p>Step 3. <strong>Calculate one or several metrics for each
partition</strong>, to be used as the basis for evaluation
plots.</p></li>
<li><p>Step 4. <strong>Search for one or several optimal number(s) of
clusters using evaluation plots</strong>. Different criteria can be
applied to identify the optimal number(s) of clusters.</p></li>
<li><p>Step 5. <strong>Export the optimal partitions from your cluster
object.</strong></p></li>
</ol>
<div class="section level3">
<h3 id="a-practical-example">3.1 A practical example<a class="anchor" aria-label="anchor" href="#a-practical-example"></a>
</h3>
<p>In this example we will compute the evaluation metric used by Holt et
al. (2013), which compares the total dissimilarity of the distance
matrix (sum of all distances) with the inter-cluster dissimilarity (sum
of distances between clusters). Then we will choose the optimal number
of clusters as the elbow of the evaluation plot.</p>
<div class="sourceCode" id="cb42"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate dissimilarities</span></span>
<span><span class="va">dissim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dissimilarity.html">dissimilarity</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 1 &amp; 2. Compute the tree and cut it into many different partitions</span></span>
<span><span class="va">tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim</span>,</span>
<span>                          n_clust <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Randomizing the dissimilarity matrix with 30 trials</span></span></code></pre>
<pre><code><span><span class="co">##  -- range of cophenetic correlation coefficients among</span></span>
<span><span class="co">##                      trials: 0.68 - 0.7</span></span></code></pre>
<pre><code><span><span class="co">## Optimal tree has a 0.7 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<div class="sourceCode" id="cb46"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 3. Calculate the same evaluation metric as Holt et al. 2013</span></span>
<span><span class="va">eval_tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/partition_metrics.html">partition_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                                dissimilarity <span class="op">=</span> <span class="va">dissim</span>, <span class="co"># Provide distances to compute the metrics</span></span>
<span>                                eval_metric <span class="op">=</span> <span class="st">"pc_distance"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 4. Find the optimal number of clusters</span></span>
<span><span class="va">opti_n_tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the elbow method</span></span></code></pre>
<pre><code><span><span class="co">##    * elbow found at:</span></span></code></pre>
<pre><code><span><span class="co">## pc_distance 15</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-17-1.png" width="700"></p>
<div class="sourceCode" id="cb56"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">opti_n_tree4</span></span></code></pre></div>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  elbow </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 15</span></span></code></pre>
<div class="sourceCode" id="cb58"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 5. Extract the optimal number of clusters</span></span>
<span><span class="co"># We get the name of the correct partition in the next line</span></span>
<span><span class="va">K_name</span> <span class="op">&lt;-</span> <span class="va">opti_n_tree4</span><span class="op">$</span><span class="va">evaluation_df</span><span class="op">$</span><span class="va">K</span><span class="op">[</span><span class="va">opti_n_tree4</span><span class="op">$</span><span class="va">evaluation_df</span><span class="op">$</span><span class="va">optimal_n_pc_distance</span><span class="op">]</span></span>
<span><span class="co"># Look at the site-cluster table</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">tree4</span><span class="op">$</span><span class="va">clusters</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ID"</span>, <span class="va">K_name</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##      ID K_15</span></span>
<span><span class="co">## 959 959    1</span></span>
<span><span class="co">## 719 719    2</span></span>
<span><span class="co">## 85   85    3</span></span>
<span><span class="co">## 285 285    3</span></span>
<span><span class="co">## 38   38    3</span></span>
<span><span class="co">## 158 158    2</span></span></code></pre>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make a map of the clusters</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegesf</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-spatial.github.io/sf/" class="external-link">sf</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Linking to GEOS 3.10.2, GDAL 3.4.1, PROJ 8.2.1; sf_use_s2() is TRUE</span></span></code></pre>
<div class="sourceCode" id="cb62"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/map_clusters.html">map_clusters</a></span><span class="op">(</span><span class="va">tree4</span><span class="op">$</span><span class="va">clusters</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ID"</span>, <span class="va">K_name</span><span class="op">)</span><span class="op">]</span>,</span>
<span>             <span class="va">vegesf</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-17-2.png" width="700"></p>
<p>Or if you are allergic to lines of code, you could also simply recut
your tree at the identified optimal number of cut-offs with
<code><a href="../reference/cut_tree.html">cut_tree()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="evaluation-metrics">3.2 Evaluation metrics<a class="anchor" aria-label="anchor" href="#evaluation-metrics"></a>
</h3>
<p>Currently, there are four evaluation metrics available in the
package:</p>
<ol style="list-style-type: decimal">
<li><p><code>pc_distance</code>: <span class="math inline">\(\sum{between-cluster\beta_{sim }} /
\sum{\beta_{sim}}\)</span> This metric is the metric computed in <span class="citation">Holt et al. (2013)</span>.</p></li>
<li><p><code>anosim</code>: this the statistic used in Analysis of
Similarities, as suggested in <span class="citation">Castro-Insua et al.
(2018)</span>. It compares the between-cluster dissimilarities to the
within-cluster dissimilarities. It is based based on the difference of
mean ranks between groups and within groups with the following formula:
<span class="math inline">\(R=(r_B-r_W)/(N(N-1)/4)\)</span> where <span class="math inline">\(r_B\)</span> and <span class="math inline">\(r_W\)</span> are the average ranks between and
within clusters respectively, and <span class="math inline">\(N\)</span>
is the total number of sites.</p></li>
<li><p><code>avg_endemism</code>: it is the average percentage of
endemism in clusters <span class="citation">(Kreft &amp; Jetz,
2010)</span>. It is calculated as follows: <span class="math inline">\(End_{mean} = \frac{\sum_{i=1}^K E_i /
S_i}{K}\)</span> where <span class="math inline">\(E_i\)</span> is the
number of endemic species in cluster <span class="math inline">\(i\)</span>, <span class="math inline">\(S_i\)</span> is the number of species in cluster
<span class="math inline">\(i\)</span>, and <span class="math inline">\(K\)</span> the maximum number of
clusters.</p></li>
<li><p><code>tot_endemism</code>: it is the total endemism across all
clusters <span class="citation">(Kreft &amp; Jetz, 2010)</span>. It is
calculated as follows: <span class="math inline">\(End_{tot} = E /
C\)</span> where <span class="math inline">\(E\)</span> is the total
number of endemic species (i.e., species occurring in only one cluster)
and <span class="math inline">\(C\)</span> is the number of non- endemic
species.</p></li>
</ol>
<p><strong>Important note</strong></p>
<p>To be able to calculate <code>pc_distance</code> and
<code>anosim</code>, you need to provide your dissimilarity object to
the argument <code>dissimilarity</code>. In addition, to be able to
calculate <code>avg_endemism</code> and <code>tot_endemism</code>, you
need to provide your species-site network to the argument
<code>net</code>. Let’s see that in practice. Depending on the size of
your dataset, computing endemism-based metrics can take a while.</p>
<div class="sourceCode" id="cb63"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate pc_distance and anosim</span></span>
<span><span class="fu"><a href="../reference/partition_metrics.html">partition_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                  dissimilarity <span class="op">=</span> <span class="va">dissim</span>, </span>
<span>                  eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pc_distance"</span>, <span class="st">"anosim"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<pre><code><span><span class="co">##   - anosim OK</span></span></code></pre>
<pre><code><span><span class="co">## Partition metrics:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Requested metric(s):  pc_distance anosim </span></span>
<span><span class="co">##  - Metric summary:</span></span>
<span><span class="co">##      pc_distance    anosim</span></span>
<span><span class="co">## Min    0.5189075 0.6962932</span></span>
<span><span class="co">## Mean   0.8953154 0.8062399</span></span>
<span><span class="co">## Max    0.9789798 0.8627211</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Access the data.frame of metrics with your_object$evaluation_df</span></span></code></pre>
<div class="sourceCode" id="cb68"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate avg_endemism and tot_endemism</span></span>
<span><span class="co"># I have an abundance matrix, I need to convert it into network format first:</span></span>
<span><span class="va">vegenet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mat_to_net.html">mat_to_net</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/partition_metrics.html">partition_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                  net <span class="op">=</span> <span class="va">vegenet</span>, </span>
<span>                  eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing composition-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - avg_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">##   - tot_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">## Partition metrics:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Requested metric(s):  avg_endemism tot_endemism </span></span>
<span><span class="co">##  - Metric summary:</span></span>
<span><span class="co">##      avg_endemism tot_endemism</span></span>
<span><span class="co">## Min   0.001185444   0.05004057</span></span>
<span><span class="co">## Mean  0.008815836   0.08120152</span></span>
<span><span class="co">## Max   0.184233194   0.31593184</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Access the data.frame of metrics with your_object$evaluation_df</span></span>
<span><span class="co">## Details of endemism % for each partition are available in </span></span>
<span><span class="co">##         your_object$endemism_results</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="criteria-to-choose-an-optimal-number-of-clusters">3.2 Criteria to choose an optimal number of clusters<a class="anchor" aria-label="anchor" href="#criteria-to-choose-an-optimal-number-of-clusters"></a>
</h3>
<p>Choosing the optimal number of clusters is a long-standing issue in
the literature and there is no absolute and objective answer to this
question. A plethora of methods have been proposed over the years, and
the best approach to tackle this issue is probably to compare the
results of multiple approaches to make an informed decision.</p>
<p>In the bioregion package, we have implemented several methods which
are suitable for bioregionalisation analyses specifically.</p>
<p>For example, a standard criterion used to identify the optimal number
of clusters is the <strong>elbow method</strong>, which is the default
criterion in <code><a href="../reference/find_optimal_n.html">find_optimal_n()</a></code>. However, we recommend moving
beyond the paradigm of a single optimal number of clusters, which is
likely an oversimplification of the hierarchy of the tree. We recommend
considering multiple cuts of the tree, and we provide several methods to
do that: <strong>identifying large steps in the curve</strong> or
<strong>using multiple cutoffs</strong>. Additionally, we implement
other approaches such as using the maximum or minimum value of metrics,
or, experimentally, using a MARS model (multiple adaptive regression
splines) to identify cutoff points in the curve.</p>
<p>Before we head into these different methods, we will compute all
evaluation metrics and store them into <code>eval_tree4</code>:</p>
<div class="sourceCode" id="cb73"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vegenet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mat_to_net.html">mat_to_net</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span><span class="va">eval_tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/partition_metrics.html">partition_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                                dissimilarity <span class="op">=</span> <span class="va">dissim</span>, </span>
<span>                                net <span class="op">=</span> <span class="va">vegenet</span>, </span>
<span>                                eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pc_distance"</span>, <span class="st">"anosim"</span>,</span>
<span>                                                <span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<pre><code><span><span class="co">##   - anosim OK</span></span></code></pre>
<pre><code><span><span class="co">## Computing composition-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - avg_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">##   - tot_endemism OK</span></span></code></pre>
<div class="section level4">
<h4 id="elbow-method">3.2.1 Elbow method<a class="anchor" aria-label="anchor" href="#elbow-method"></a>
</h4>
<p>The elbow method consists in find the ‘elbow’ in the form of the
metric-cluster relationship. This method will typically work for metrics
which have an L-shaped form (typically, pc_distance and endemism
metrics), but not for other metrics (e.g. the form of anosim does not
necessarily follow a L-shape).</p>
<p><em>The rationale behind the elbow method is to find a cut-off above
which the metric values stop to increase significantly, such that adding
new clusters does not provide much significant improvement in the
metric.</em></p>
<p>The elbow method is the default method in
<code><a href="../reference/find_optimal_n.html">find_optimal_n()</a></code>. There are no parameters to adjust it.
When the curve is not elbow-shaped, it may give spurious results.</p>
<div class="sourceCode" id="cb80"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the elbow method</span></span></code></pre>
<pre><code><span><span class="co">##    * elbow found at:</span></span></code></pre>
<pre><code><span><span class="co">## pc_distance 15</span></span>
<span><span class="co">## anosim 15</span></span>
<span><span class="co">## avg_endemism 9</span></span>
<span><span class="co">## tot_endemism 10</span></span></code></pre>
<pre><code><span><span class="co">## Warning in find_optimal_n(eval_tree4): The elbow method is likely not suitable</span></span>
<span><span class="co">## for the ANOSIM metric. You should rather look for leaps in the curve (see</span></span>
<span><span class="co">## criterion = 'increasing_step' or decreasing_step)</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-20-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  elbow </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 15</span></span>
<span><span class="co">## anosim - 15</span></span>
<span><span class="co">## avg_endemism - 9</span></span>
<span><span class="co">## tot_endemism - 10</span></span></code></pre>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-21-1.png" width="700"></p>
<p>In our example above, the optimal number of clusters varies depending
in the metric, from a minimum of 9 to a maximum of 15. The final choice
depends on your preferences with respect to metrics, and your objectives
with the clustering. Alternatively, two cut-offs could be used, a deep
cut-off based on endemism metrics at e.g. a value of 9, and a shallow
cutoff based on <code>pc_distance</code>, at 15.</p>
</div>
<div class="section level4">
<h4 id="step-method">3.2.2 Step method<a class="anchor" aria-label="anchor" href="#step-method"></a>
</h4>
<p>The step method consists in identifying the largest “steps” in
metrics, i.e., largest increases or decreases in the value of the
metric.</p>
<p>To do this, the function will compute all the consecutive differences
in metrics between partitions. It will then keep only the largest
positive differences (<code>increasing_step</code>) or negative
differences (<code>decreasing_step</code>). <code>increasing_step</code>
is for increasing metrics (<code>pc_distance</code>) and
<code>decreasing_step</code> is for decreasing metrics
(<code>avg_endemism</code> and <code>tot_endemism</code>).
<code>anosim</code> values can either increase or decrease depending on
your dataset, so you would have to explore both ways.</p>
<p>By default, the function will select the top 1% steps:</p>
<div class="sourceCode" id="cb90"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"anosim"</span>, <span class="st">"pc_distance"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"increasing_step"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "anosim"      "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the increasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-22-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  anosim pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  increasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  increase  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## anosim - 15</span></span>
<span><span class="co">## pc_distance - 6</span></span></code></pre>
<div class="sourceCode" id="cb97"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"decreasing_step"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the decreasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-22-2.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  decreasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  decrease  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## avg_endemism - 3</span></span>
<span><span class="co">## tot_endemism - 6</span></span></code></pre>
<p>However, you can adjust it in two different ways. First, choose a
number of steps to select, e.g. to select the largest 3 steps, use
<code>step_levels = 3</code>:</p>
<div class="sourceCode" id="cb104"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"anosim"</span>, <span class="st">"pc_distance"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"increasing_step"</span>,</span>
<span>               step_levels <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "anosim"      "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the increasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-23-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  anosim pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  increasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  increase  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## anosim - 6 15 96</span></span>
<span><span class="co">## pc_distance - 3 6 15</span></span></code></pre>
<p>Second, you can set a quantile of steps to select, e.g. to select the
5% largest steps set the quantile to 0.90
(<code>step_quantile = 0.95</code>):</p>
<div class="sourceCode" id="cb111"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"anosim"</span>, <span class="st">"pc_distance"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"increasing_step"</span>,</span>
<span>               step_quantile <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "anosim"      "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the increasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-24-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  anosim pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  increasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.95  (i.e., only the top 5 %  increase  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## anosim - 6 15 31 47 96</span></span>
<span><span class="co">## pc_distance - 3 6 15 31 47</span></span></code></pre>
<p>Finally, one question that may arise is which cluster number to
select when a large step happens. For example, if the largest step
occurs between partitions with 4 and 5 clusters, should we keep the
partition with 4 clusters, or the partition with 5 clusters?</p>
<p>By default, the function will keep the partition <span class="math inline">\(N + 1\)</span> (5 clusters in our example above).
You can change that by setting
<code>step_round_above = FALSE</code>:</p>
<div class="sourceCode" id="cb118"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"decreasing_step"</span>,</span>
<span>               step_round_above <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the decreasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-25-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  decreasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  decrease  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## avg_endemism - 2</span></span>
<span><span class="co">## tot_endemism - 5</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cutting-at-different-cut-off-values">3.2.3 Cutting at different cut-off values<a class="anchor" aria-label="anchor" href="#cutting-at-different-cut-off-values"></a>
</h4>
<p>The idea of this method is to select specific metric values at which
the number of clusters should be used. For example, in their study, Holt
et al. (2013) used different cutoffs on <code>pc_distance</code> to find
the global biogeographical regions: 0.90, 0.95, 0.99, 0.999. The higher
the value, the more -diversity is explained but also the more clusters
there are. Therefore, the choice is a trade-off between the total
-diversity explained and the number of clusters.</p>
<p>Eventually, the choice of these values depends on different
factors:</p>
<ol style="list-style-type: decimal">
<li><p>The geographical scale of your study. Global scale study can use
large cutoffs like Holt et al. (2013) and end up with a reasonable
amount of clusters, whereas in regional to local-scale studies with less
endemism and more taxa shared among clusters, these values are too high,
and other cutoffs should be explored, such as 0.5 and 0.75.</p></li>
<li><p>The characteristics of your study which will increase or decrease
the degree of endemism among clusters: dispersal capacities of your
taxonomic group, the connectivity/barriers in your study area, etc. Use
lower cutoffs when you have a large number of widespread species, use
higher cutoffs when you have high degrees of endemism.</p></li>
<li><p>The use of abundance or phylogenetic data to compute -diversity
metrics may give a better distinction of clusters, which in turn will
allow you to use higher cutoffs.</p></li>
</ol>
<p>In our case, a regional scale study on vegetation, we can for example
use three cutoffs: 0.6 (deep cutoff), 0.8 (intermediate cutoff) and 0.9
(shallow cutoff).</p>
<div class="sourceCode" id="cb125"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="st">"pc_distance"</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"cutoff"</span>,</span>
<span>               metric_cutoffs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.6</span>, <span class="fl">.8</span>, <span class="fl">.9</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the cutoff method</span></span></code></pre>
<pre><code><span><span class="co">##  - Cutoff method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-26-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  cutoff </span></span>
<span><span class="co">##    --&gt; cutoff(s) chosen:  0.6 0.8 0.9 </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 6 15 43</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cutting-at-the-maximum-or-minimum-metric-value">3.2.4 Cutting at the maximum or minimum metric value<a class="anchor" aria-label="anchor" href="#cutting-at-the-maximum-or-minimum-metric-value"></a>
</h4>
<p>This criterion will find the maximum (<code>criterion = "max"</code>)
or minimum (<code>criterion = "min"</code>) value of the metric in the
list of partitions and select the associated partition. Such a criterion
can be interesting in the case of <code>anosim</code>, but is probably
much less useful for the other metrics implemented in the package.</p>
</div>
<div class="section level4">
<h4 id="finding-break-points-in-the-curve">3.2.5 Finding break points in the curve<a class="anchor" aria-label="anchor" href="#finding-break-points-in-the-curve"></a>
</h4>
<p>This criterion consists in applying a segmented regression model with
the formula evaluation metric ~ number of clusters. The user can define
the number of breaks to identify on the curve. Note that such a model
likely requires a minimum number of points to find an appropriate number
of clusters. In our example here, we make 100 cuts in the tree to have
enough values.</p>
<div class="sourceCode" id="cb132"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree5</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree4</span>,</span>
<span>                  cut_height <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fl">0.67</span>, <span class="co"># We cut the tree 100 times from 0 to 0.67 </span></span>
<span>                                   length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span> <span class="co"># (0.67 is the maximum height of my tree here)</span></span>
<span></span>
<span><span class="va">eval_tree5</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/partition_metrics.html">partition_metrics</a></span><span class="op">(</span><span class="va">tree5</span>, </span>
<span>                                dissimilarity <span class="op">=</span> <span class="va">dissim</span>, </span>
<span>                                net <span class="op">=</span> <span class="va">vegenet</span>, </span>
<span>                                eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pc_distance"</span>, <span class="st">"anosim"</span>,</span>
<span>                                                <span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<pre><code><span><span class="co">##   - anosim OK</span></span></code></pre>
<pre><code><span><span class="co">## Computing composition-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - avg_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">##   - tot_endemism OK</span></span></code></pre>
<div class="sourceCode" id="cb139"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree5</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"breakpoints"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 100</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the breakpoints method</span></span></code></pre>
<pre><code><span><span class="co">## Exact break point not in the list of partitions: finding the closest partition...</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<pre><code><span><span class="co">##    (the red line is the prediction from the segmented regression)</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-27-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 100  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  701 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  breakpoints </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 19</span></span>
<span><span class="co">## anosim - 105</span></span>
<span><span class="co">## avg_endemism - 6</span></span>
<span><span class="co">## tot_endemism - 8</span></span></code></pre>
<p>We can ask for a higher number of breaks:</p>
<ul>
<li>2 breaks</li>
</ul>
<div class="sourceCode" id="cb147"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree5</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"breakpoints"</span>,</span>
<span>               n_breakpoints <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 100</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the breakpoints method</span></span></code></pre>
<pre><code><span><span class="co">## Exact break point not in the list of partitions: finding the closest partition...</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<pre><code><span><span class="co">##    (the red line is the prediction from the segmented regression)</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-28-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 100  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  701 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  breakpoints </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 8 61</span></span>
<span><span class="co">## anosim - 16 165</span></span>
<span><span class="co">## avg_endemism - 6 23</span></span>
<span><span class="co">## tot_endemism - 8 105</span></span></code></pre>
<ul>
<li>3 breaks</li>
</ul>
<div class="sourceCode" id="cb155"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree5</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"breakpoints"</span>,</span>
<span>               n_breakpoints <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 100</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the breakpoints method</span></span></code></pre>
<pre><code><span><span class="co">## Exact break point not in the list of partitions: finding the closest partition...</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<pre><code><span><span class="co">##    (the red line is the prediction from the segmented regression)</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-29-1.png" width="700"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 100  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  701 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  breakpoints </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 7 19 88</span></span>
<span><span class="co">## anosim - 16 139 311</span></span>
<span><span class="co">## avg_endemism - 5 15 55</span></span>
<span><span class="co">## tot_endemism - 8 83 165</span></span></code></pre>
<p>Increasing the number of breaks can be useful in situations where you
have, for example, non-linear silhouettes of metric ~ n clusters.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="optics-as-a-semi-hierarchical-clustering-approach">4. OPTICS as a semi-hierarchical clustering approach<a class="anchor" aria-label="anchor" href="#optics-as-a-semi-hierarchical-clustering-approach"></a>
</h2>
<p>OPTICS (Ordering Points To Identify the Clustering Structure) is a
semi- hierarchical clustering approach which orders points in the
datasets such that points which are closest become neighbours,
calculates a ‘reachability’ distance between each point and then extract
clusters from this reachability distance in a hierarchical manner.
However, the hierarchical nature of clusters is not directly provided by
the algorithm in a tree-like output. Hence, users should explore the
‘reachability plot’ to understand the hierarchical nature of their
OPTICS clusters, and read the associated publication to further grasp
this method <span class="citation">(Hahsler et al., 2019)</span>.</p>
<p>To run the optics algorithm, use the <code><a href="../reference/hclu_optics.html">hclu_optics()</a></code>
function:</p>
<div class="sourceCode" id="cb163"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate dissimilarities</span></span>
<span><span class="va">dissim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dissimilarity.html">dissimilarity</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="va">clust1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_optics.html">hclu_optics</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span>
<span></span>
<span><span class="va">clust1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : optics </span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">## Clustering results:</span></span>
<span><span class="co">##  - Number of partitions:  1 </span></span>
<span><span class="co">##  - Number of clusters:  9</span></span></code></pre>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-Baselga2012" class="csl-entry">
Baselga, A. (2012). The relationship between species replacement,
dissimilarity derived from nestedness, and nestedness. <em>Global
Ecology and Biogeography</em>, <em>21</em>(12), 1223–1232.
</div>
<div id="ref-Baselga2013" class="csl-entry">
Baselga, A. (2013). Separating the two components of abundance-based
dissimilarity: Balanced changes in abundance vs. Abundance gradients.
<em>Methods in Ecology and Evolution</em>, <em>4</em>(6), 552–557.
</div>
<div id="ref-Castro-Insua2018" class="csl-entry">
Castro-Insua, A., Gómez-Rodríguez, C., &amp; Baselga, A. (2018). <span class="nocase">Dissimilarity measures affected by richness differences
yield biased delimitations of biogeographic realms</span>. <em>Nature
Communications</em>, <em>9</em>(1), 9–11.
</div>
<div id="ref-Ficetola2017" class="csl-entry">
Ficetola, G. F., Mazel, F., &amp; Thuiller, W. (2017). <span class="nocase">Global determinants of zoogeographical boundaries</span>.
<em>Nature Ecology &amp; Evolution</em>, <em>1</em>, 0089.
</div>
<div id="ref-Hahsler2019" class="csl-entry">
Hahsler, M., Piekenbrock, M., &amp; Doran, D. (2019). Dbscan: Fast
density-based clustering with r. <em>Journal of Statistical
Software</em>, <em>91</em>(1).
</div>
<div id="ref-Holt2013" class="csl-entry">
Holt, B. G., Lessard, J.-P., Borregaard, M. K., Fritz, S. A., Araújo, M.
B., Dimitrov, D., Fabre, P.-H., Graham, C. H., Graves, G. R., Jønsson,
K. a., Nogués-Bravo, D., Wang, Z., Whittaker, R. J., Fjeldså, J., &amp;
Rahbek, C. (2013). <span class="nocase">An update of Wallace’s
zoogeographic regions of the world</span>. <em>Science</em>,
<em>339</em>(6115), 74–78.
</div>
<div id="ref-Kreft2010" class="csl-entry">
Kreft, H., &amp; Jetz, W. (2010). <span class="nocase">A framework for
delineating biogeographical regions based on species
distributions</span>. <em>Journal of Biogeography</em>, <em>37</em>,
2029–2053.
</div>
<div id="ref-Leprieur2014" class="csl-entry">
Leprieur, F., &amp; Oikonomou, A. (2014). <span class="nocase">The need
for richness-independent measures of turnover when delineating
biogeographical regions</span>. <em>Journal of Biogeography</em>,
<em>41</em>, 417–420.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p></p>
<p>Developed by Maxime Lenormand, Boris Leroy, Pierre Denelle.</p>
</div>

<div class="pkgdown-footer-right">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.0.7.</p>
</div>

    </footer>
</div>

  

  

  </body>
</html>
