<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>4.1 Hierarchical clustering • bioregion</title>
<!-- favicons --><link rel="icon" type="image/png" sizes="16x16" href="../favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="../favicon-32x32.png">
<link rel="apple-touch-icon" type="image/png" sizes="180x180" href="../apple-touch-icon.png">
<link rel="apple-touch-icon" type="image/png" sizes="120x120" href="../apple-touch-icon-120x120.png">
<link rel="apple-touch-icon" type="image/png" sizes="76x76" href="../apple-touch-icon-76x76.png">
<link rel="apple-touch-icon" type="image/png" sizes="60x60" href="../apple-touch-icon-60x60.png">
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><script src="../deps/MathJax-3.2.2/tex-chtml.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="4.1 Hierarchical clustering">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">bioregion</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">1.1.3</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../articles/bioregion.html">Get started</a></li>
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/a1_install_binary_files.html">1. Installation of the binary files</a></li>
    <li><a class="dropdown-item" href="../articles/a2_matrix_and_network_formats.html">2. Matrix and network formats</a></li>
    <li><a class="dropdown-item" href="../articles/a3_pairwise_metrics.html">3. Pairwise similarity/dissimilarity metrics</a></li>
    <li><a class="dropdown-item" href="../articles/a4_1_hierarchical_clustering.html">4.1 Hierarchical clustering</a></li>
    <li><a class="dropdown-item" href="../articles/a4_2_non_hierarchical_clustering.html">4.2 Non-hierarchical clustering</a></li>
    <li><a class="dropdown-item" href="../articles/a4_3_network_clustering.html">4.3 Network clustering</a></li>
    <li><a class="dropdown-item" href="../articles/a4_4_microbenchmark.html">4.4 Microbenchmark</a></li>
    <li><a class="dropdown-item" href="../articles/a5_1_visualization.html">5.1 Visualization</a></li>
    <li><a class="dropdown-item" href="../articles/a5_2_compare_bioregionalizations.html">5.2 Compare bioregionalizations</a></li>
    <li><a class="dropdown-item" href="../articles/a5_3_summary_metrics.html">5.3 Summary metrics</a></li>
  </ul>
</li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-news" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">News</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-news">
<li><h6 class="dropdown-header" data-toc-skip>Releases</h6></li>
    <li><a class="external-link dropdown-item" href="https://github.com/bioRgeo/bioregion/releases/tag/v1.1.1">Version 1.1.1</a></li>
    <li><a class="external-link dropdown-item" href="https://github.com/bioRgeo/bioregion/releases/tag/v1.1.0">Version 1.1.0</a></li>
    <li><a class="external-link dropdown-item" href="https://github.com/bioRgeo/bioregion/releases/tag/v1.0.0">Version 1.0.0</a></li>
    <li><hr class="dropdown-divider"></li>
    <li><a class="dropdown-item" href="../news/index.html">Changelog</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/bioRgeo/bioregion/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">




<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>4.1 Hierarchical clustering</h1>
                        <h4 data-toc-skip class="author">Boris Leroy,
Maxime Lenormand and Pierre Denelle</h4>
            
            <h4 data-toc-skip class="date">2024-12-16</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/bioRgeo/bioregion/blob/master/vignettes/a4_1_hierarchical_clustering.Rmd" class="external-link"><code>vignettes/a4_1_hierarchical_clustering.Rmd</code></a></small>
      <div class="d-none name"><code>a4_1_hierarchical_clustering.Rmd</code></div>
    </div>

    
    
<p>Hierarchical clustering consists in creating a hierarchical tree from
a matrix of distances (or beta-diversities). From this hierarchical
tree, clusters can be obtained by cutting the tree.</p>
<p>Although these methods are conceptually simple, their implementation
can be complex and requires important user decisions. Here, we provide a
step-by-step guide to performing hierarchical clustering analyses with
<code>bioregion</code>, along with comments on the philosophy on how we
designed the functions.</p>
<p>Hierarchical clustering takes place on the right-hand side of the
<code>bioregion</code> conceptual diagram:</p>
<center>
<img align="bottom" width="100%" height="100%" src="../reference/figures/workflow_nonetwork.png">
</center>
<div class="section level2">
<h2 id="compute-dissimilarity-indices-from-input-data">1. Compute dissimilarity indices from input data<a class="anchor" aria-label="anchor" href="#compute-dissimilarity-indices-from-input-data"></a>
</h2>
<p>To initiate the hierarchical clustering procedure, you need to
provide pairwise distances between sites. These pairwise distances
between sites can be obtained by running <code><a href="../reference/dissimilarity.html">dissimilarity()</a></code> on
a species-site matrix, such as a presence-absence or an abundance
matrix.</p>
<p>In the example below, we use the vegetation dataset from the package
to compute distance metrics.</p>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://github.com/bioRgeo/bioregion" class="external-link">"bioregion"</a></span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Work with the vegetation dataset we include in the package</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># This is an abundance matrix where sites are in rows and species in columns</span></span>
<span><span class="va">vegemat</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">10</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">10</span><span class="op">]</span></span></code></pre></div>
<pre><code><span><span class="co">##     Species</span></span>
<span><span class="co">## Site 10001 10002 10003 10004 10005 10006 10007 10008 10009 10010</span></span>
<span><span class="co">##   35     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   36     2     0     0     0     0     0     1    12     0     0</span></span>
<span><span class="co">##   37     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   38     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   39     5     0     0     0     0     0     0     2     0     0</span></span>
<span><span class="co">##   84     0     0     0     0     0     0     0     0     0     0</span></span>
<span><span class="co">##   85     3     0     0     0     0     0     1     7     0     0</span></span>
<span><span class="co">##   86     0     0     0     2     0     0     2    22     0     0</span></span>
<span><span class="co">##   87    16     0     0     0     0     0     2    54     0     0</span></span>
<span><span class="co">##   88   228     0     0     0     0     0     0     5     0     0</span></span></code></pre>
<p>We are going to compute the <span class="math inline">\(\beta_{sim}\)</span> diversity metric, which is a
presence-absence dissimilarity index. The formula is as follows: <span class="math inline">\(\beta_{sim} = min(b, c) / (a+min(b,
c))\)</span></p>
<p>Where <em>a</em> is the number of species shared by both sites;
<em>b</em> is the number of species occurring only in the first site;
and <em>c</em> is the number of species only occurring only in the
second site.</p>
<p>We typically choose this metric for bioregionalization, because it is
the <strong>turnover</strong> component of the Sorensen index <span class="citation">(Baselga, 2012)</span> (in a nutshell, it tells us how
sites are different because they have distinct species), and because it
is less dependent on species richness than the Jaccard turnover <span class="citation">(Leprieur &amp; Oikonomou, 2014)</span>. Alternatively,
given that we have abundance data here, we could also use the
Bray-Curtis turnover index <span class="citation">(Baselga,
2013)</span>. The choice of the distance metric is very important for
the outcome of the clustering procedure, so we recommend that you choose
carefully depending on your research question.</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">dissim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dissimilarity.html">dissimilarity</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Data.frame of dissimilarity between sites</span></span>
<span><span class="co">##  - Total number of sites:  715 </span></span>
<span><span class="co">##  - Total number of species:  3697 </span></span>
<span><span class="co">##  - Number of rows:  255255 </span></span>
<span><span class="co">##  - Number of dissimilarity metrics:  1 </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## </span></span>
<span><span class="co">##   Site1 Site2    Simpson</span></span>
<span><span class="co">## 2    35    36 0.02325581</span></span>
<span><span class="co">## 3    35    37 0.03100775</span></span>
<span><span class="co">## 4    35    38 0.05426357</span></span>
<span><span class="co">## 5    35    39 0.05426357</span></span>
<span><span class="co">## 6    35    84 0.72093023</span></span>
<span><span class="co">## 7    35    85 0.08527132</span></span></code></pre>
<p>By default, only the Simpson index is computed, but other options are
available in the <code>metric</code> argument of dissimilarity().
Furthermore, users can also write down their own formula to compute any
index they want for in the argument <code>formula</code>, see
?dissimilarity().</p>
<p>We are now ready to start the hierarchical clustering procedure with
the object <code>dissim</code> we have just created. Alternatively, you
can also use other types of objects in <code><a href="../reference/hclu_hierarclust.html">hclu_hierarclust()</a></code>,
such as a distance matrix object (class <code>dist</code>) or a
<code>data.frame</code> of your own crafting (make sure to read the
required format carefully in <code><a href="../reference/hclu_hierarclust.html">?hclu_hierarclust</a></code>).</p>
</div>
<div class="section level2">
<h2 id="hierarchical-clustering-with-basic-parameters">2. Hierarchical clustering with basic parameters<a class="anchor" aria-label="anchor" href="#hierarchical-clustering-with-basic-parameters"></a>
</h2>
<p>Hierarchical clustering, and the associated hierarchical tree, can be
constructed in two ways: - <em>agglomerative</em>, where all sites are
initially assigned to their own bioregion and they are progressively
grouped together - <em>divisive</em>, where all sites initially belong
to the same unique bioregion and are then progressively divided into
different bioregions</p>
<p>Subsections 2.1 to 2.3 detail the functioning of agglomerative
hierarchical clustering, while <a href="#id_2.4.">sub-section 2.4.</a>
illustrates the divisive method.</p>
<div class="section level3">
<h3 id="basic-usage">2.1 Basic usage<a class="anchor" aria-label="anchor" href="#basic-usage"></a>
</h3>
<p>The basic use of the function is as follows:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Building the iterative hierarchical consensus tree... Note that this process can take time especially if you have a lot of sites.</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Final tree has a 0.6863 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<p>The function gives us some information as it proceeds. Notably, it
talks about a randomization of the dissimilarity matrix - this is a very
important feature because hierarchical clustering is strongly influenced
by the order of the sites in the distance matrix. Therefore, by default,
the function performs a randomization of the order of sites in the
distance matrix with 30 trials (<a href="#randomization">more
information in the randomization section</a>). It also tells us that
among all the trials selected the tree with the highest cophenetic
correlation coefficient, with a value of 0.69.</p>
<p>We can see type the name of the object in the console to see more
information:</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hclu_hierarclust </span></span>
<span><span class="co">##  (hierarchical clustering based on a dissimilarity matrix)</span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">##  - Name of dissimilarity metric:  Simpson </span></span>
<span><span class="co">##  - Tree construction method:  average </span></span>
<span><span class="co">##  - Randomization of the dissimilarity matrix:  yes, number of trials 100 </span></span>
<span><span class="co">##  - Method to compute the final tree:  Iterative consensus hierarchical tree </span></span>
<span><span class="co">##  - Cophenetic correlation coefficient:  0.686 </span></span>
<span><span class="co">## Clustering procedure incomplete - no clusters yet</span></span></code></pre>
<p>The last line tells us that the the clustering procedure is
incomplete: the tree has been built, but it has not yet been cut - so
there are no clusters in the object yet.</p>
<p>To cut the tree, we can use the <code><a href="../reference/cut_tree.html">cut_tree()</a></code> function:</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ask for 3 clusters</span></span>
<span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree1</span>,</span>
<span>                  n_clust <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Determining the cut height to reach 3 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.5625</span></span></code></pre>
<p>Here, we asked for 3 clusters, and the algorithm automatically finds
the height at which 3 clusters are found (h = 0.562).</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hclu_hierarclust </span></span>
<span><span class="co">##  (hierarchical clustering based on a dissimilarity matrix)</span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">##  - Name of dissimilarity metric:  Simpson </span></span>
<span><span class="co">##  - Tree construction method:  average </span></span>
<span><span class="co">##  - Randomization of the dissimilarity matrix:  yes, number of trials 100 </span></span>
<span><span class="co">##  - Method to compute the final tree:  Iterative consensus hierarchical tree </span></span>
<span><span class="co">##  - Cophenetic correlation coefficient:  0.686 </span></span>
<span><span class="co">##  - Number of clusters requested by the user:  3 </span></span>
<span><span class="co">## Clustering results:</span></span>
<span><span class="co">##  - Number of partitions:  1 </span></span>
<span><span class="co">##  - Number of clusters:  3 </span></span>
<span><span class="co">##  - Height of cut of the hierarchical tree: 0.562</span></span></code></pre>
<p>When we type again the name of the object in the console, it gives us
the results of the clustering: we have</p>
<ul>
<li>
<strong>1 partition</strong>: a partition is a clustering result. We
only cut the tree once, so we only have 1 partition at the moment</li>
<li>
<strong>4 clusters</strong>: this is the number of clusters in the
partition. We asked for 3, and we obtained 3, which is good. Sometimes,
however, we cannot get the number of clusters we asked for - in which
case the outcome will be indicated.</li>
<li>
<strong>a height of cut at 0.562</strong>: this is the height of cut
at which we can obtain 4 clusters in our tree.</li>
</ul>
<p>We can make a quick plot of our partitioned tree with</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># We reduced the size of text labels with cex = .2, because there are too many sites</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree1</span>, cex <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-7-1.png" width="1152"></p>
<p>Let’s see how it looks like on a map:</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegesf</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://r-spatial.github.io/sf/" class="external-link">sf</a></span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Linking to GEOS 3.12.1, GDAL 3.8.4, PROJ 9.4.0; sf_use_s2() is TRUE</span></span></code></pre>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/map_bioregions.html">map_bioregions</a></span><span class="op">(</span><span class="va">tree1</span><span class="op">$</span><span class="va">clusters</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ID"</span>, <span class="st">"K_3"</span><span class="op">)</span><span class="op">]</span>, <span class="va">vegesf</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-8-1.png" width="576"></p>
<p>Now, this is a hierarchical tree, and cutting it only once (= only 1
partition) oversimplifies the result of the tree. Why not cut it
multiple times? For example, we could make deep, intermediate, and
shallow cuts to the tree, likewise to <span class="citation">Ficetola et
al. (2017)</span>, which would allow us to see broad- to fine-scale
relationships among sites in our tree.</p>
<p>We can specify, e.g. 4, 10 and 20 clusters:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Ask for 4, 10 and 20 clusters</span></span>
<span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree1</span>,</span>
<span>                  n_clust <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">2</span>, <span class="fl">3</span>, <span class="fl">12</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Determining the cut height to reach 2 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.625</span></span></code></pre>
<pre><code><span><span class="co">## Determining the cut height to reach 3 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.5625</span></span></code></pre>
<pre><code><span><span class="co">## Determining the cut height to reach 12 groups...</span></span></code></pre>
<pre><code><span><span class="co">## --&gt; 0.4453125</span></span></code></pre>
<div class="sourceCode" id="cb26"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree1</span>, cex <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-9-1.png" width="1152">
However, it may be more useful to choose the heights of cut, rather than
the number of clusters. We could, for example, cut the tree at heights
0.4 (shallow cut), 0.5 (intermediate cut) and 0.6 (deep cut):</p>
<div class="sourceCode" id="cb27"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree1</span>,</span>
<span>                  cut_height <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.4</span>, <span class="fl">.5</span>, <span class="fl">.6</span><span class="op">)</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree1</span>, cex <span class="op">=</span> <span class="fl">.2</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-10-1.png" width="1152"></p>
<p>The plot is not easy to read because of the large number of sites. We
can rather extract the information directly from the object:</p>
<div class="sourceCode" id="cb28"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hclu_hierarclust </span></span>
<span><span class="co">##  (hierarchical clustering based on a dissimilarity matrix)</span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">##  - Name of dissimilarity metric:  Simpson </span></span>
<span><span class="co">##  - Tree construction method:  average </span></span>
<span><span class="co">##  - Randomization of the dissimilarity matrix:  yes, number of trials 100 </span></span>
<span><span class="co">##  - Method to compute the final tree:  Iterative consensus hierarchical tree </span></span>
<span><span class="co">##  - Cophenetic correlation coefficient:  0.686 </span></span>
<span><span class="co">##  - Heights of cut requested by the user:  0.4 0.5 0.6 </span></span>
<span><span class="co">## Clustering results:</span></span>
<span><span class="co">##  - Number of partitions:  3 </span></span>
<span><span class="co">##  - Partitions are not hierarchical</span></span>
<span><span class="co">##  - Number of clusters:  2 6 23 </span></span>
<span><span class="co">##  - Height of cut of the hierarchical tree: 0.6 0.5 0.4</span></span></code></pre>
<p>From the result, we can read that for the deep cut partition (h =
0.6) we have clusters, for the intermediate cut partition (h = 0.5) we
have 6 clusters and for the shallow cut partition (h = 0.4) we have 23
clusters.</p>
<p>Here is how the maps look like:</p>
<div class="sourceCode" id="cb30"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw">for</span><span class="op">(</span><span class="va">i</span> <span class="kw">in</span> <span class="fl">2</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">ncol</a></span><span class="op">(</span><span class="va">tree1</span><span class="op">$</span><span class="va">clusters</span><span class="op">)</span><span class="op">)</span><span class="op">{</span></span>
<span>  <span class="fu"><a href="../reference/map_bioregions.html">map_bioregions</a></span><span class="op">(</span><span class="va">tree1</span><span class="op">$</span><span class="va">clusters</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">1</span>, <span class="va">i</span><span class="op">)</span><span class="op">]</span>, <span class="va">vegesf</span><span class="op">)</span></span>
<span><span class="op">}</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-12-1.png" width="576"><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-12-2.png" width="576"><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-12-3.png" width="576"></p>
<p>In the next section we will see what are the default settings and why
we chose them, and then we will see how to find optimal numbers of
clusters.</p>
</div>
<div class="section level3">
<h3 id="exploring-the-outputs">2.2 Exploring the outputs<a class="anchor" aria-label="anchor" href="#exploring-the-outputs"></a>
</h3>
<p>To explore the object, you can use <code><a href="https://rdrr.io/r/utils/str.html" class="external-link">str()</a></code> to see the
object structure:</p>
<div class="sourceCode" id="cb31"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/str.html" class="external-link">str</a></span><span class="op">(</span><span class="va">tree1</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## List of 6</span></span>
<span><span class="co">##  $ name        : chr "hclu_hierarclust"</span></span>
<span><span class="co">##  $ args        :List of 14</span></span>
<span><span class="co">##   ..$ index              : chr "Simpson"</span></span>
<span><span class="co">##   ..$ method             : chr "average"</span></span>
<span><span class="co">##   ..$ randomize          : logi TRUE</span></span>
<span><span class="co">##   ..$ n_runs             : num 100</span></span>
<span><span class="co">##   ..$ optimal_tree_method: chr "iterative_consensus_tree"</span></span>
<span><span class="co">##   ..$ keep_trials        : logi FALSE</span></span>
<span><span class="co">##   ..$ n_clust            : NULL</span></span>
<span><span class="co">##   ..$ cut_height         : num [1:3] 0.4 0.5 0.6</span></span>
<span><span class="co">##   ..$ find_h             : logi TRUE</span></span>
<span><span class="co">##   ..$ h_max              : num 1</span></span>
<span><span class="co">##   ..$ h_min              : num 0</span></span>
<span><span class="co">##   ..$ consensus_p        : num 0.5</span></span>
<span><span class="co">##   ..$ verbose            : logi TRUE</span></span>
<span><span class="co">##   ..$ dynamic_tree_cut   : logi FALSE</span></span>
<span><span class="co">##  $ inputs      :List of 7</span></span>
<span><span class="co">##   ..$ bipartite      : logi FALSE</span></span>
<span><span class="co">##   ..$ weight         : logi TRUE</span></span>
<span><span class="co">##   ..$ pairwise       : logi TRUE</span></span>
<span><span class="co">##   ..$ pairwise_metric: chr "Simpson"</span></span>
<span><span class="co">##   ..$ dissimilarity  : logi TRUE</span></span>
<span><span class="co">##   ..$ nb_sites       : int 715</span></span>
<span><span class="co">##   ..$ hierarchical   : logi FALSE</span></span>
<span><span class="co">##  $ algorithm   :List of 6</span></span>
<span><span class="co">##   ..$ final.tree         :List of 5</span></span>
<span><span class="co">##   .. ..- attr(*, "class")= chr "hclust"</span></span>
<span><span class="co">##   ..$ final.tree.coph.cor: num 0.686</span></span>
<span><span class="co">##   ..$ final.tree.msd     : num 0.023</span></span>
<span><span class="co">##   ..$ trials             : chr "Trials not stored in output"</span></span>
<span><span class="co">##   ..$ output_n_clust     : Named int [1:3] 2 6 23</span></span>
<span><span class="co">##   .. ..- attr(*, "names")= chr [1:3] "h_0.6" "h_0.5" "h_0.4"</span></span>
<span><span class="co">##   ..$ output_cut_height  : num [1:3] 0.6 0.5 0.4</span></span>
<span><span class="co">##  $ clusters    :'data.frame':    715 obs. of  4 variables:</span></span>
<span><span class="co">##   ..$ ID  : chr [1:715] "1003" "1004" "1005" "1006" ...</span></span>
<span><span class="co">##   ..$ K_2 : chr [1:715] "1" "1" "1" "1" ...</span></span>
<span><span class="co">##   ..$ K_6 : chr [1:715] "1" "1" "1" "1" ...</span></span>
<span><span class="co">##   ..$ K_23: chr [1:715] "1" "1" "1" "1" ...</span></span>
<span><span class="co">##  $ cluster_info:'data.frame':    3 obs. of  3 variables:</span></span>
<span><span class="co">##   ..$ partition_name      : chr [1:3] "K_2" "K_6" "K_23"</span></span>
<span><span class="co">##   ..$ n_clust             : int [1:3] 2 6 23</span></span>
<span><span class="co">##   ..$ requested_cut_height: num [1:3] 0.6 0.5 0.4</span></span>
<span><span class="co">##  - attr(*, "class")= chr [1:2] "bioregion.clusters" "list"</span></span></code></pre>
<p>It show you the different slots in the object, and how you can access
them. For example, if I want to access the <code>clusters</code> slot, I
have to type <code>tree1$clusters</code>.</p>
<ul>
<li>
<strong>name</strong>: the name of the method we are using</li>
<li>
<strong>args</strong>: the arguments you have selected for your
tree</li>
<li>
<strong>inputs</strong>: this is mostly for internal use in the
package, it provides some info about the nature of input data and
methods</li>
<li>
<strong>algorithm</strong>: this slot contains detailed information
about the hierarchical clustering. For example, you can have access to
the raw tree here, in <code>hclust</code> format. To access it, I can
type <code>tree1$algorithm$final.tree</code>
</li>
<li>
<strong>clusters</strong>: this is a <code>data.frame</code>
containing your partitions. The first column is your sites, and all the
other columns are the partitions.</li>
<li>
<strong>cluster_info</strong>: this is a small
<code>data.frame</code> which will help you link your requests with the
<code>clusters</code> <code>data.frame</code>. Its content varies
depending on your choices; for example, in my case, it looks like
this:</li>
</ul>
<div class="sourceCode" id="cb33"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree1</span><span class="op">$</span><span class="va">cluster_info</span></span></code></pre></div>
<pre><code><span><span class="co">##       partition_name n_clust requested_cut_height</span></span>
<span><span class="co">## h_0.6            K_2       2                  0.6</span></span>
<span><span class="co">## h_0.5            K_6       6                  0.5</span></span>
<span><span class="co">## h_0.4           K_23      23                  0.4</span></span></code></pre>
<p>It shows the name of the partition (corresponding to column names in
<code>tree1$clusters</code>), the number of clusters in each partition,
and the cut height I initially requested.</p>
</div>
<div class="section level3">
<h3 id="explanation-of-the-default-settings-and-how-to-change-them">2.3 Explanation of the default settings and how to change them<a class="anchor" aria-label="anchor" href="#explanation-of-the-default-settings-and-how-to-change-them"></a>
</h3>
<div class="section level4">
<h4 id="randomization">2.3.1 Randomization of the distance matrix<a class="anchor" aria-label="anchor" href="#randomization"></a>
</h4>
<p>The order of sites in the distance matrix influences the outcome of
the hierarchical tree. Let’s see that with an example:</p>
<div class="sourceCode" id="cb35"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute the tree without randomizing the distance matrix</span></span>
<span><span class="va">tree2</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim</span>,</span>
<span>                          randomize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Output tree has a 0.6849 cophenetic correlation coefficient with the initial</span></span>
<span><span class="co">##                    dissimilarity matrix</span></span></code></pre>
<div class="sourceCode" id="cb37"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree2</span>, cex <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-15-1.png" width="768"></p>
<p>This is how the tree looks like when the matrix is not randomized.
Now let’s randomize it and regenerate the tree:</p>
<div class="sourceCode" id="cb38"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># This line randomizes the order of rows in the distance matrix</span></span>
<span><span class="va">dissim_random</span> <span class="op">&lt;-</span> <span class="va">dissim</span><span class="op">[</span><span class="fu"><a href="https://rdrr.io/r/base/sample.html" class="external-link">sample</a></span><span class="op">(</span><span class="fl">1</span><span class="op">:</span><span class="fu"><a href="https://rdrr.io/r/base/nrow.html" class="external-link">nrow</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span><span class="op">)</span>, <span class="op">]</span></span>
<span></span>
<span><span class="co"># Recompute the tree</span></span>
<span><span class="va">tree3</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim_random</span>,</span>
<span>                          randomize <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Output tree has a 0.6778 cophenetic correlation coefficient with the initial</span></span>
<span><span class="co">##                    dissimilarity matrix</span></span></code></pre>
<div class="sourceCode" id="cb40"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree3</span>, cex <span class="op">=</span> <span class="fl">.1</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-16-1.png" width="768"></p>
<p>See how the tree looks different? This is problematic because it
means that the outcome is heavily influenced by the order of sites in
the distance matrix.</p>
<p>To address this issue, we have developed an iterative algorithm that
will reconstruct the entire tree from top to bottom, by selecting for
each branch a majority decision among multiple randomizations of the
distance matrix (100 times by default, can be increased). This method is
called <strong>Iterative Hierarchical Consensus Tree</strong> (argument
<code>optimal_tree_method = "iterative_consensus_tree"</code>, default
value) and it ensures that you obtain a consensus tree that it will find
a majority decision for each branch of the tree. The tree produced with
this method generally have a better topology than any individual tree.
We estimate the performance of the topology with the <strong>cophenetic
correlation coefficient</strong>, which is the <em>correlation between
the initial distance <span class="math inline">\(\beta_{sim}\)</span>
among sites</em> and <em>the cophenetic distance</em>, which is the
distance at which sites are connected in the tree. It tells us how
representative is the tree of the initial distance matrix.</p>
<p>Although this method performs better than any other available method,
it comes with a computing cost: it needs to randomize the distance
matrix multiple times for each branching of the tree. Therefore, we
recommend using it to obtain a robust tree - be patient in that case.
Otherwise, if you only need a very fast look at a <em>good</em> tree,
you can simply select the single best tree among multiple randomization
trials. It will never be as good as the <strong>Iterative Hierarchical
Consensus Tree</strong>, but it will be a best choice for a fast
exploration. To do that, choose
<code>optimal_tree_method = "best"</code>. It will select the tree that
best represents the distance matrix; i.e., the one that has the highest
cophenetic correlation coefficient among all trials.</p>
<p>Let’s see an example of <code>optimal_tree_method = "best"</code>. We
can also ask the function to keep all individual trees for further
exploration (<code>keep_trials = TRUE</code>).</p>
<div class="sourceCode" id="cb41"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree_best</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim_random</span>,</span>
<span>                              randomize <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                              optimal_tree_method <span class="op">=</span> <span class="st">"best"</span>,</span>
<span>                              keep_trials <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Randomizing the dissimilarity matrix with 100 trials</span></span></code></pre>
<pre><code><span><span class="co">##  -- range of cophenetic correlation coefficients among trials: 0.6669 - 0.6974</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Final tree has a 0.6974 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<p>Another possible approach is to build a simple consensus tree among
all the trials. However, we generally do not recommend constructing a
consensus tree, because the topology of simple consensus trees can be
very problematic if there are a lot of ties in the distance matrix.
Let’s see it in action here:</p>
<div class="sourceCode" id="cb45"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree_consensus</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim_random</span>,</span>
<span>                                   randomize <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>                                   optimal_tree_method <span class="op">=</span> <span class="st">"consensus"</span>,</span>
<span>                                   keep_trials <span class="op">=</span> <span class="cn">TRUE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Randomizing the dissimilarity matrix with 100 trials</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Final tree has a 0.202 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<p>See how the cophenetic correlation coefficient for the consensus tree
is terrible compared to the IHCT and best tree ?</p>
<p>This consensus tree has almost no correlation with our initial
distance matrix. This is because its topology is terribly wrong, see how
the tree looks like:</p>
<div class="sourceCode" id="cb48"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree_consensus</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-19-1.png" width="768"></p>
<p>Booo, this is just a large rake, not a tree!!!</p>
</div>
<div class="section level4">
<h4 id="tree-construction-algorithm">2.3.2 Tree construction algorithm<a class="anchor" aria-label="anchor" href="#tree-construction-algorithm"></a>
</h4>
<p>By default, the function uses the UPGMA method (Unweighted Pair Group
Method with Arithmetic Mean) because it has been recommended in
bioregionalization for its better performance over other approaches
<span class="citation">(Kreft &amp; Jetz, 2010)</span>. You can change
this method by changing the argument <code>method</code>; all methods
implemented in <code><a href="https://rdrr.io/r/stats/hclust.html" class="external-link">stats::hclust()</a></code> are available.</p>
<p>Note that the current height distances for methods
<code>method = "ward.D"</code> and <code>method = "ward.D2"</code> may
differ from the calculations in <code><a href="https://rdrr.io/r/stats/hclust.html" class="external-link">stats::hclust()</a></code>, due to the
iterative nature of the algorithm. In addition, using the method
<code>method = "single"</code> are much slower than the other
approaches, and we have not yet implemented a workaround to make it
faster (do not hesitate to contact us if you need a faster
implementation or have an idea of how to make it run faster).</p>
</div>
<div class="section level4">
<h4 id="cutting-the-tree">2.3.3 Cutting the tree<a class="anchor" aria-label="anchor" href="#cutting-the-tree"></a>
</h4>
<p>There are three ways of cutting the tree:</p>
<ol style="list-style-type: decimal">
<li>
<strong>Specify the expected number of clusters</strong>: you can
request a specific number of clusters (<code>n_clust = 5</code> for
example). You can also request multiple partitions, each with their own
number of clusters (<code>n_clust = c(5, 10, 15)</code>) for
example.</li>
</ol>
<p><strong>Note:</strong> When you specify the number of clusters, the
function will search for the associated height of cut automatically; you
can disable this parameter with <code>find_h = FALSE</code>. It will
search for this <em>h</em> value between <code>h_max</code> (default 1)
and <code>h_min</code> (default 0). These arguments can be adjusted if
you are working with indices whose values do not range between 0 and
1.</p>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>Specify the height of cut</strong>: you can request the
height at which you want to cut the tree (e.g.,
<code>cut_height = 0.5</code>). You can also request multiple
partitions, each with their own cut height
(<code>cut_height = c(0.4, 0.5, 0.6)</code>) for example.</p></li>
<li><p><strong>Use a dynamic tree cut method</strong>: Rather than
cutting the entire tree at once, this alternative approach consists in
cutting individual branches at different heights. This method can be
requested by using <code>dynamic_tree_cut = TRUE</code>, and is based on
the dynamicTreeCut R package.</p></li>
</ol>
</div>
</div>
<div class="section level3">
<h3 id="id_2-4-">2.4. Divisive clustering<a class="anchor" aria-label="anchor" href="#id_2-4-"></a>
</h3>
<p>While the agglomerative hierarchical clustering in the previous
subsections followed a bottom-up approach, divisive clustering follows a
top-down approach. This means that in the first step of the clustering,
all sites belong to the same bioregion, and then sites are iteratively
divided into different bioregions until all sites belong to a unique
bioregion.</p>
<p>Divisive clustering is following the DIvisive ANAlysis (DIANA)
clustering algorithm described in <span class="citation">(Kaufman &amp;
Rousseeuw, 2009)</span>.</p>
<p>At each step, the algorithm splits the largest cluster by identifying
the most dissimilar observation (i.e. site) and then putting sites that
are closer to this most dissimilar observation than to the ‘old party’
group into a splinter group. The result is that the large cluster is
split into two clusters.</p>
<p>The function <code>hclu_diana</code> performs the Diana divisive
clustering.</p>
<div class="sourceCode" id="cb49"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Compute the tree with the Diana algorithm</span></span>
<span><span class="va">tree_diana</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_diana.html">hclu_diana</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Output tree has a 0.41 cophenetic correlation coefficient with the initial</span></span>
<span><span class="co">##                    dissimilarity matrix</span></span></code></pre>
<div class="sourceCode" id="cb51"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">tree_diana</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-20-1.png" width="768"></p>
</div>
</div>
<div class="section level2">
<h2 id="optimaln">3. How to find an optimal number of clusters?<a class="anchor" aria-label="anchor" href="#optimaln"></a>
</h2>
<center>
<img align="bottom" width="100%" height="100%" src="../reference/figures/find_optimal_n.png">
</center>
<ol style="list-style-type: decimal">
<li><p>Step 1. <strong>Build a tree</strong> with
<code><a href="../reference/hclu_hierarclust.html">hclu_hierarclust()</a></code></p></li>
<li><p>Step 2. <strong>Explore a range of partitions</strong>, from a
minimum (e.g., starting at 2 clusters) up to a maximum (e.g. <span class="math inline">\(n-1\)</span> clusters where <span class="math inline">\(n\)</span> is the number of sites).</p></li>
<li><p>Step 3. <strong>Calculate one or several metrics for each
partition</strong>, to be used as the basis for evaluation
plots.</p></li>
<li><p>Step 4. <strong>Search for one or several optimal number(s) of
clusters using evaluation plots</strong>. Different criteria can be
applied to identify the optimal number(s) of clusters.</p></li>
<li><p>Step 5. <strong>Export the optimal partitions from your cluster
object.</strong></p></li>
</ol>
<div class="section level3">
<h3 id="a-practical-example">3.1 A practical example<a class="anchor" aria-label="anchor" href="#a-practical-example"></a>
</h3>
<p>In this example we will compute the evaluation metric used by <span class="citation">Holt et al. (2013)</span>, which compares the total
dissimilarity of the distance matrix (sum of all distances) with the
inter-cluster dissimilarity (sum of distances between clusters). Then we
will choose the optimal number of clusters as the elbow of the
evaluation plot.</p>
<div class="sourceCode" id="cb52"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate dissimilarities</span></span>
<span><span class="va">dissim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dissimilarity.html">dissimilarity</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Step 1 &amp; 2. Compute the tree and cut it into many different partitions</span></span>
<span><span class="va">tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_hierarclust.html">hclu_hierarclust</a></span><span class="op">(</span><span class="va">dissim</span>,</span>
<span>                          n_clust <span class="op">=</span> <span class="fl">2</span><span class="op">:</span><span class="fl">100</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Building the iterative hierarchical consensus tree... Note that this process can take time especially if you have a lot of sites.</span></span></code></pre>
<pre><code><span><span class="co">## </span></span>
<span><span class="co">## Final tree has a 0.6842 cophenetic correlation coefficient with the initial dissimilarity</span></span>
<span><span class="co">##       matrix</span></span></code></pre>
<pre><code><span><span class="co">## Warning in cut_tree(outputs, n_clust = n_clust, cut_height = cut_height, : The requested number of cluster could not be found</span></span>
<span><span class="co">##                          for k = 4. Closest number found: 5</span></span></code></pre>
<pre><code><span><span class="co">## Warning in cut_tree(outputs, n_clust = n_clust, cut_height = cut_height, : The requested number of cluster could not be found</span></span>
<span><span class="co">##                          for k = 67. Closest number found: 66</span></span></code></pre>
<div class="sourceCode" id="cb57"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 3. Calculate the same evaluation metric as Holt et al. 2013</span></span>
<span><span class="va">eval_tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bioregionalization_metrics.html">bioregionalization_metrics</a></span><span class="op">(</span></span>
<span>  <span class="va">tree4</span>, </span>
<span>  dissimilarity <span class="op">=</span> <span class="va">dissim</span>, <span class="co"># Provide distances to compute the metrics</span></span>
<span>  eval_metric <span class="op">=</span> <span class="st">"pc_distance"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<div class="sourceCode" id="cb60"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 4. Find the optimal number of clusters</span></span>
<span><span class="va">opti_n_tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the elbow method</span></span></code></pre>
<pre><code><span><span class="co">##    * elbow found at:</span></span></code></pre>
<pre><code><span><span class="co">## pc_distance 19</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-22-1.png" width="768"></p>
<div class="sourceCode" id="cb67"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">opti_n_tree4</span></span></code></pre></div>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  elbow </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 19</span></span></code></pre>
<div class="sourceCode" id="cb69"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Step 5. Extract the optimal number of clusters</span></span>
<span><span class="co"># We get the name of the correct partition in the next line</span></span>
<span><span class="va">K_name</span> <span class="op">&lt;-</span> <span class="va">opti_n_tree4</span><span class="op">$</span><span class="va">evaluation_df</span><span class="op">$</span><span class="va">K</span><span class="op">[</span><span class="va">opti_n_tree4</span><span class="op">$</span><span class="va">evaluation_df</span><span class="op">$</span><span class="va">optimal_n_pc_distance</span><span class="op">]</span></span>
<span><span class="co"># Look at the site-cluster table</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/head.html" class="external-link">head</a></span><span class="op">(</span><span class="va">tree4</span><span class="op">$</span><span class="va">clusters</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ID"</span>, <span class="va">K_name</span><span class="op">)</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">##        ID K_19</span></span>
<span><span class="co">## 1003 1003    1</span></span>
<span><span class="co">## 1004 1004    1</span></span>
<span><span class="co">## 1005 1005    1</span></span>
<span><span class="co">## 1006 1006    1</span></span>
<span><span class="co">## 1007 1007    1</span></span>
<span><span class="co">## 1008 1008    1</span></span></code></pre>
<div class="sourceCode" id="cb71"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Make a map of the clusters</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="st">"vegesf"</span><span class="op">)</span></span>
<span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="st"><a href="https://r-spatial.github.io/sf/" class="external-link">"sf"</a></span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/map_bioregions.html">map_bioregions</a></span><span class="op">(</span><span class="va">tree4</span><span class="op">$</span><span class="va">clusters</span><span class="op">[</span>, <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"ID"</span>, <span class="va">K_name</span><span class="op">)</span><span class="op">]</span>, <span class="va">vegesf</span><span class="op">)</span></span></code></pre></div>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-22-2.png" width="768"></p>
<p>Or if you are allergic to lines of code, you could also simply recut
your tree at the identified optimal number of cut-offs with
<code><a href="../reference/cut_tree.html">cut_tree()</a></code>.</p>
</div>
<div class="section level3">
<h3 id="evaluation-metrics">3.2 Evaluation metrics<a class="anchor" aria-label="anchor" href="#evaluation-metrics"></a>
</h3>
<p>Currently, there are four evaluation metrics available in the
package:</p>
<ol style="list-style-type: decimal">
<li><p><code>pc_distance</code>: <span class="math inline">\(\sum{between-cluster\beta_{sim }} /
\sum{\beta_{sim}}\)</span> This metric is the metric computed in <span class="citation">Holt et al. (2013)</span>.</p></li>
<li><p><code>anosim</code>: this the statistic used in Analysis of
Similarities, as suggested in <span class="citation">Castro-Insua et al.
(2018)</span>. It compares the between-cluster dissimilarities to the
within-cluster dissimilarities. It is based on the difference of mean
ranks between groups and within groups with the following formula: <span class="math inline">\(R=(r_B-r_W)/(N(N-1)/4)\)</span> where <span class="math inline">\(r_B\)</span> and <span class="math inline">\(r_W\)</span> are the average ranks between and
within clusters respectively, and <span class="math inline">\(N\)</span>
is the total number of sites.</p></li>
<li><p><code>avg_endemism</code>: it is the average percentage of
endemism in clusters <span class="citation">(Kreft &amp; Jetz,
2010)</span>. It is calculated as follows: <span class="math inline">\(End_{mean} = \frac{\sum_{i=1}^K E_i /
S_i}{K}\)</span> where <span class="math inline">\(E_i\)</span> is the
number of endemic species in cluster <span class="math inline">\(i\)</span>, <span class="math inline">\(S_i\)</span> is the number of species in cluster
<span class="math inline">\(i\)</span>, and <span class="math inline">\(K\)</span> the maximum number of
clusters.</p></li>
<li><p><code>tot_endemism</code>: it is the total endemism across all
clusters <span class="citation">(Kreft &amp; Jetz, 2010)</span>. It is
calculated as follows: <span class="math inline">\(End_{tot} = E /
C\)</span> where <span class="math inline">\(E\)</span> is the total
number of endemic species (i.e., species occurring in only one cluster)
and <span class="math inline">\(C\)</span> is the number of non- endemic
species.</p></li>
</ol>
<p><strong>Important note</strong></p>
<p>To be able to calculate <code>pc_distance</code> and
<code>anosim</code>, you need to provide your dissimilarity object to
the argument <code>dissimilarity</code>. In addition, to be able to
calculate <code>avg_endemism</code> and <code>tot_endemism</code>, you
need to provide your species-site network to the argument
<code>net</code> (don’t panick if you only have a species x site matrix!
We have a function to make the conversion).</p>
<p>Let’s see that in practice. Depending on the size of your dataset,
computing endemism-based metrics can take a while.</p>
<div class="sourceCode" id="cb72"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate pc_distance and anosim</span></span>
<span><span class="fu"><a href="../reference/bioregionalization_metrics.html">bioregionalization_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                           dissimilarity <span class="op">=</span> <span class="va">dissim</span>, </span>
<span>                           eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pc_distance"</span>, <span class="st">"anosim"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<pre><code><span><span class="co">##   - anosim OK</span></span></code></pre>
<pre><code><span><span class="co">## Partition metrics:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Requested metric(s):  pc_distance anosim </span></span>
<span><span class="co">##  - Metric summary:</span></span>
<span><span class="co">##      pc_distance    anosim</span></span>
<span><span class="co">## Min    0.4982831 0.6968281</span></span>
<span><span class="co">## Mean   0.8950634 0.8029604</span></span>
<span><span class="co">## Max    0.9823024 0.8669999</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Access the data.frame of metrics with your_object$evaluation_df</span></span></code></pre>
<div class="sourceCode" id="cb77"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Calculate avg_endemism and tot_endemism</span></span>
<span><span class="co"># I have an abundance matrix, I need to convert it into network format first:</span></span>
<span><span class="va">vegenet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mat_to_net.html">mat_to_net</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="fu"><a href="../reference/bioregionalization_metrics.html">bioregionalization_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                           net <span class="op">=</span> <span class="va">vegenet</span>, </span>
<span>                           eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing composition-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - avg_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">##   - tot_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">## Partition metrics:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Requested metric(s):  avg_endemism tot_endemism </span></span>
<span><span class="co">##  - Metric summary:</span></span>
<span><span class="co">##      avg_endemism tot_endemism</span></span>
<span><span class="co">## Min   0.001177433   0.04895862</span></span>
<span><span class="co">## Mean  0.008144637   0.08297200</span></span>
<span><span class="co">## Max   0.181513150   0.31539086</span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Access the data.frame of metrics with your_object$evaluation_df</span></span>
<span><span class="co">## Details of endemism % for each partition are available in </span></span>
<span><span class="co">##         your_object$endemism_results</span></span></code></pre>
</div>
<div class="section level3">
<h3 id="criteria-to-choose-an-optimal-number-of-clusters">3.2 Criteria to choose an optimal number of clusters<a class="anchor" aria-label="anchor" href="#criteria-to-choose-an-optimal-number-of-clusters"></a>
</h3>
<p>Choosing the optimal number of clusters is a long-standing issue in
the literature, and there is no absolute and objective answer to this
question. A plethora of methods have been proposed over the years, and
the best approach to tackle this issue is probably to compare the
results of multiple approaches to make an informed decision.</p>
<p>In the <code>bioregion</code> package, we have implemented several
methods that are specifically suited for bioregionalization
analysis.</p>
<p>For example, a standard criterion used for identifying the optimal
number of clusters is the <strong>elbow method</strong>, which is the
default criterion in <code><a href="../reference/find_optimal_n.html">find_optimal_n()</a></code>. However, we
recommend moving beyond the paradigm of a single optimal number of
clusters, which is likely an oversimplification of the hierarchy of the
tree. We recommend considering multiple cuts of the tree, and we provide
several methods for doing so: <strong>identifying large steps in the
curve</strong> or <strong>using multiple cutoffs</strong>. Additionally,
we implement other approaches, such as using the maximum or minimum
value of the metrics, or by finding break points in the curve with a
segmented model.</p>
<p>Before we look at these different methods, we will compute all the
evaluation metrics and store them in <code>eval_tree4</code>:</p>
<div class="sourceCode" id="cb82"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">vegenet</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/mat_to_net.html">mat_to_net</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span><span class="va">eval_tree4</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bioregionalization_metrics.html">bioregionalization_metrics</a></span><span class="op">(</span><span class="va">tree4</span>, </span>
<span>                                         dissimilarity <span class="op">=</span> <span class="va">dissim</span>, </span>
<span>                                         net <span class="op">=</span> <span class="va">vegenet</span>, </span>
<span>                                         eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pc_distance"</span>, <span class="st">"anosim"</span>,</span>
<span>                                                         <span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<pre><code><span><span class="co">##   - anosim OK</span></span></code></pre>
<pre><code><span><span class="co">## Computing composition-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - avg_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">##   - tot_endemism OK</span></span></code></pre>
<div class="section level4">
<h4 id="elbow-method">3.2.1 Elbow method<a class="anchor" aria-label="anchor" href="#elbow-method"></a>
</h4>
<p>The elbow method consists in find the ‘elbow’ in the form of the
metric-cluster relationship. This method will typically work for metrics
which have an L-shaped form (typically, pc_distance and endemism
metrics), but not for other metrics (e.g. the form of anosim does not
necessarily follow an L-shape).</p>
<p><em>The rationale behind the elbow method is to find a cutoff above
which the metric values stop increasing significantly, such that adding
new clusters does not provide much significant improvement in the
metric.</em></p>
<p>The elbow method is the default method in
<code><a href="../reference/find_optimal_n.html">find_optimal_n()</a></code>. There are no parameters to adjust it. If
the curve is not elbow-shaped, it may give spurious results.</p>
<div class="sourceCode" id="cb89"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the elbow method</span></span></code></pre>
<pre><code><span><span class="co">##    * elbow found at:</span></span></code></pre>
<pre><code><span><span class="co">## pc_distance 19</span></span>
<span><span class="co">## anosim 24</span></span>
<span><span class="co">## avg_endemism 9</span></span>
<span><span class="co">## tot_endemism 12</span></span></code></pre>
<pre><code><span><span class="co">## Warning in find_optimal_n(eval_tree4): The elbow method is likely not suitable</span></span>
<span><span class="co">## for the ANOSIM metric. You should rather look for leaps in the curve (see</span></span>
<span><span class="co">## criterion = 'increasing_step' or decreasing_step)</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-25-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  elbow </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 19</span></span>
<span><span class="co">## anosim - 24</span></span>
<span><span class="co">## avg_endemism - 9</span></span>
<span><span class="co">## tot_endemism - 12</span></span></code></pre>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-26-1.png" width="480"></p>
<p>In our example above, the optimal number of clusters varies depending
on the metric, from a minimum of 9 to a maximum of 24. The final choice
depends on your metric preferences with respect to metrics, and your
objectives with the clustering. Alternatively, two cut-offs could be
used, a deep cut-off based on the endemism metrics e.g. at a value of 9,
and a shallow cutoff based on <code>pc_distance</code>, at 19.</p>
</div>
<div class="section level4">
<h4 id="step-method">3.2.2 Step method<a class="anchor" aria-label="anchor" href="#step-method"></a>
</h4>
<p>The step method consists in identifying the largest “steps” in
metrics, i.e., the largest increases or decreases in the value of the
metric.</p>
<p>To do this, the function calculates all successive differences in
metrics between partitions. It will then keep only the largest positive
differences (<code>increasing_step</code>) or negative differences
(<code>decreasing_step</code>). <code>increasing_step</code> is for
increasing metrics (<code>pc_distance</code>) and
<code>decreasing_step</code> is for decreasing metrics
(<code>avg_endemism</code> and <code>tot_endemism</code>).
<code>anosim</code> values can either increase or decrease depending on
your dataset, so you would have to explore both ways.</p>
<p>By default, the function selects the top 1% steps:</p>
<div class="sourceCode" id="cb99"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"anosim"</span>, <span class="st">"pc_distance"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"increasing_step"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "anosim"      "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the increasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-27-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  anosim pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  increasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  increase  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## anosim - 19</span></span>
<span><span class="co">## pc_distance - 12</span></span></code></pre>
<div class="sourceCode" id="cb106"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"decreasing_step"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the decreasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-27-2.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  decreasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  decrease  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## avg_endemism - 5</span></span>
<span><span class="co">## tot_endemism - 5</span></span></code></pre>
<p>However, you can adjust it in two different ways. First, choose a
number of steps to select, e.g. to select the largest 3 steps, use
<code>step_levels = 3</code>:</p>
<div class="sourceCode" id="cb113"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"anosim"</span>, <span class="st">"pc_distance"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"increasing_step"</span>,</span>
<span>               step_levels <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "anosim"      "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the increasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-28-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  anosim pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  increasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  increase  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## anosim - 5 12 19</span></span>
<span><span class="co">## pc_distance - 5 12 19</span></span></code></pre>
<p>Note that these steps generally correspond to large jumps in the
tree, which is why we like this approach as it fits well with the
hierarchical nature of the tree.</p>
<p>Second, you can set a quantile of steps to select, e.g. to select the
5% largest steps set the quantile to 0.95
(<code>step_quantile = 0.95</code>):</p>
<div class="sourceCode" id="cb120"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"anosim"</span>, <span class="st">"pc_distance"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"increasing_step"</span>,</span>
<span>               step_quantile <span class="op">=</span> <span class="fl">0.95</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "anosim"      "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the increasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-29-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  anosim pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  increasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.95  (i.e., only the top 5 %  increase  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## anosim - 5 12 19 24 93</span></span>
<span><span class="co">## pc_distance - 3 5 12 19 68</span></span></code></pre>
<p>Finally, a question that may arise is which cluster number to select
when a large step occurs. For example, if the largest step occurs
between partitions with 4 and 5 clusters, should we keep the partition
with 4 clusters, or the partition with 5 clusters?</p>
<p>By default, the function keeps the partition with <span class="math inline">\(N + 1\)</span> (5 clusters in our example above).
You can change this by setting
<code>step_round_above = FALSE</code>:</p>
<div class="sourceCode" id="cb127"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"decreasing_step"</span>,</span>
<span>               step_round_above <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the decreasing_step method</span></span></code></pre>
<pre><code><span><span class="co">##  - Step method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-30-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  decreasing_step </span></span>
<span><span class="co">##    (step quantile chosen:  0.99  (i.e., only the top 1 %  decrease  in evaluation metrics  are used as break points for the number of clusters)</span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## avg_endemism - 3</span></span>
<span><span class="co">## tot_endemism - 3</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cutting-at-different-cut-off-values">3.2.3 Cutting at different cut-off values<a class="anchor" aria-label="anchor" href="#cutting-at-different-cut-off-values"></a>
</h4>
<p>The idea of this method is to select specific metric values at which
the number of clusters should be used. For example, in their study,
<span class="citation">Holt et al. (2013)</span> used different cutoffs
for <code>pc_distance</code> to find the global biogeographic regions:
0.90, 0.95, 0.99, 0.999. The higher the value, the more -diversity is
explained, but also the more clusters there are. Therefore, the choice
is a trade-off between the total -diversity explained and the number of
clusters.</p>
<p>Eventually, the choice of these values depends on different
factors:</p>
<ol style="list-style-type: decimal">
<li><p>The geographic scope of your study. A global scale study can use
large cutoffs like <span class="citation">Holt et al. (2013)</span> and
end up with a reasonable number of clusters, whereas in regional to
local scale studies with less endemism and more taxa shared among
clusters, these values are too high, and other cutoffs should be
explored, such as 0.5 and 0.75.</p></li>
<li><p>The characteristics of your study which will increase or decrease
the degree of endemism among clusters: dispersal capacities of your
taxonomic group, the connectivity/barriers in your study area, etc. Use
lower cutoffs when you have a large number of widespread species, use
higher cutoffs when you have high degrees of endemism.</p></li>
<li><p>Using abundance or phylogenetic data to compute -diversity
metrics may allow you to better distinguish clusters, which in turn will
allow you to use higher cutoffs.</p></li>
</ol>
<p>For example, in our case, a regional-scale study on vegetation, we
can use three cutoffs: 0.6 (deep cutoff), 0.8 (intermediate cutoff), and
0.9 (shallow cutoff).</p>
<div class="sourceCode" id="cb134"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree4</span>,</span>
<span>               metrics_to_use <span class="op">=</span> <span class="st">"pc_distance"</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"cutoff"</span>,</span>
<span>               metric_cutoffs <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fl">.6</span>, <span class="fl">.8</span>, <span class="fl">.9</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 99</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the cutoff method</span></span></code></pre>
<pre><code><span><span class="co">##  - Cutoff method</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-31-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 99  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  2  to  100 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  cutoff </span></span>
<span><span class="co">##    --&gt; cutoff(s) chosen:  0.6 0.8 0.9 </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 5 12 29</span></span></code></pre>
</div>
<div class="section level4">
<h4 id="cutting-at-the-maximum-or-minimum-metric-value">3.2.4 Cutting at the maximum or minimum metric value<a class="anchor" aria-label="anchor" href="#cutting-at-the-maximum-or-minimum-metric-value"></a>
</h4>
<p>This criterion finds the maximum (<code>criterion = "max"</code>) or
minimum (<code>criterion = "min"</code>) value of the metric in the list
of partitions and selects the corresponding partition. Such a criterion
can be interesting in the case of <code>anosim</code>, but is probably
much less useful for the other metrics implemented in the package.</p>
</div>
<div class="section level4">
<h4 id="finding-break-points-in-the-curve">3.2.5 Finding break points in the curve<a class="anchor" aria-label="anchor" href="#finding-break-points-in-the-curve"></a>
</h4>
<p>This criterion consists in applying a segmented regression model with
the formula evaluation metric ~ number of clusters. The user can define
the number of breaks to be identified on the curve. Note that such a
model is likely to require a minimum number of points to find an
appropriate number of clusters. In our example here, we make 100 cuts in
the tree to have enough values.</p>
<div class="sourceCode" id="cb141"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">tree5</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/cut_tree.html">cut_tree</a></span><span class="op">(</span><span class="va">tree4</span>,</span>
<span>                  cut_height <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/seq.html" class="external-link">seq</a></span><span class="op">(</span><span class="fl">0</span>, <span class="fu"><a href="https://rdrr.io/r/base/Extremes.html" class="external-link">max</a></span><span class="op">(</span><span class="va">tree4</span><span class="op">$</span><span class="va">algorithm</span><span class="op">$</span><span class="va">final.tree</span><span class="op">$</span><span class="va">height</span><span class="op">)</span>, </span>
<span>                                   length <span class="op">=</span> <span class="fl">100</span><span class="op">)</span><span class="op">)</span> </span>
<span></span>
<span><span class="va">eval_tree5</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/bioregionalization_metrics.html">bioregionalization_metrics</a></span><span class="op">(</span><span class="va">tree5</span>, </span>
<span>                                         dissimilarity <span class="op">=</span> <span class="va">dissim</span>, </span>
<span>                                         net <span class="op">=</span> <span class="va">vegenet</span>, </span>
<span>                                         eval_metric <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="st">"pc_distance"</span>, <span class="st">"anosim"</span>,</span>
<span>                                                         <span class="st">"avg_endemism"</span>, <span class="st">"tot_endemism"</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## Computing similarity-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - pc_distance OK</span></span></code></pre>
<pre><code><span><span class="co">##   - anosim OK</span></span></code></pre>
<pre><code><span><span class="co">## Computing composition-based metrics...</span></span></code></pre>
<pre><code><span><span class="co">##   - avg_endemism OK</span></span></code></pre>
<pre><code><span><span class="co">##   - tot_endemism OK</span></span></code></pre>
<div class="sourceCode" id="cb148"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree5</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"breakpoints"</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 100</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the breakpoints method</span></span></code></pre>
<pre><code><span><span class="co">## Exact break point not in the list of partitions: finding the closest partition...</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<pre><code><span><span class="co">##    (the red line is the prediction from the segmented regression)</span></span></code></pre>
<pre><code><span><span class="co">## Warning: Removed 1 row containing missing values or values outside the scale range</span></span>
<span><span class="co">## (`geom_line()`).</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-32-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 100  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  1  to  701 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  breakpoints </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 16</span></span>
<span><span class="co">## anosim - 111</span></span>
<span><span class="co">## avg_endemism - 2</span></span>
<span><span class="co">## tot_endemism - 2</span></span></code></pre>
<p>We can ask for a higher number of breaks:</p>
<ul>
<li>2 breaks</li>
</ul>
<div class="sourceCode" id="cb157"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree5</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"breakpoints"</span>,</span>
<span>               n_breakpoints <span class="op">=</span> <span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 100</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the breakpoints method</span></span></code></pre>
<pre><code><span><span class="co">## Exact break point not in the list of partitions: finding the closest partition...</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<pre><code><span><span class="co">##    (the red line is the prediction from the segmented regression)</span></span></code></pre>
<pre><code><span><span class="co">## Warning: Removed 1 row containing missing values or values outside the scale range</span></span>
<span><span class="co">## (`geom_line()`).</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-33-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 100  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  1  to  701 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  breakpoints </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 2 25</span></span>
<span><span class="co">## anosim - 25 183</span></span>
<span><span class="co">## avg_endemism - 2 16</span></span>
<span><span class="co">## tot_endemism - 2 92</span></span></code></pre>
<ul>
<li>3 breaks</li>
</ul>
<div class="sourceCode" id="cb166"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/find_optimal_n.html">find_optimal_n</a></span><span class="op">(</span><span class="va">eval_tree5</span>,</span>
<span>               criterion <span class="op">=</span> <span class="st">"breakpoints"</span>,</span>
<span>               n_breakpoints <span class="op">=</span> <span class="fl">3</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "pc_distance"  "anosim"       "avg_endemism" "tot_endemism"</span></span></code></pre>
<pre><code><span><span class="co">## Number of partitions: 100</span></span></code></pre>
<pre><code><span><span class="co">## Searching for potential optimal number(s) of clusters based on the breakpoints method</span></span></code></pre>
<pre><code><span><span class="co">## Exact break point not in the list of partitions: finding the closest partition...</span></span></code></pre>
<pre><code><span><span class="co">## Plotting results...</span></span></code></pre>
<pre><code><span><span class="co">##    (the red line is the prediction from the segmented regression)</span></span></code></pre>
<pre><code><span><span class="co">## Warning: Removed 1 row containing missing values or values outside the scale range</span></span>
<span><span class="co">## (`geom_line()`).</span></span></code></pre>
<p><img src="a4_1_hierarchical_clustering_files/figure-html/unnamed-chunk-34-1.png" width="480"></p>
<pre><code><span><span class="co">## Search for an optimal number of clusters:</span></span>
<span><span class="co">##  - 100  partition(s) evaluated</span></span>
<span><span class="co">##  - Range of clusters explored: from  1  to  701 </span></span>
<span><span class="co">##  - Evaluated metric(s):  pc_distance anosim avg_endemism tot_endemism </span></span>
<span><span class="co">## </span></span>
<span><span class="co">## Potential optimal partition(s):</span></span>
<span><span class="co">##  - Criterion chosen to optimise the number of clusters:  breakpoints </span></span>
<span><span class="co">##  - Optimal partition(s) of clusters for each metric:</span></span>
<span><span class="co">## pc_distance - 2 16 100</span></span>
<span><span class="co">## anosim - 25 183 566</span></span>
<span><span class="co">## avg_endemism - 2 13 56</span></span>
<span><span class="co">## tot_endemism - 2 14 111</span></span></code></pre>
<p>Increasing the number of breaks can be useful in situations where you
have, for example, non-linear silhouettes of metric ~ n clusters.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="optics-as-a-semi-hierarchical-clustering-approach">4. OPTICS as a semi-hierarchical clustering approach<a class="anchor" aria-label="anchor" href="#optics-as-a-semi-hierarchical-clustering-approach"></a>
</h2>
<p>OPTICS (Ordering Points To Identify the Clustering Structure) is a
semi-hierarchical clustering approach that orders the points in the
datasets so that the closest points become neighbors, calculates a
‘reachability’ distance between each point, and then extracts clusters
from this reachability distance in a hierarchical manner. However, the
hierarchical nature of clusters is not directly provided by the
algorithm in a tree-like output. Hence, users should explore the
‘reachability plot’ to understand the hierarchical nature of their
OPTICS clusters, and read the related publication to further understand
this method <span class="citation">(Hahsler et al., 2019)</span>.</p>
<p>To run the optics algorithm, use the <code><a href="../reference/hclu_optics.html">hclu_optics()</a></code>
function:</p>
<div class="sourceCode" id="cb175"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/utils/data.html" class="external-link">data</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Calculate dissimilarities</span></span>
<span><span class="va">dissim</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/dissimilarity.html">dissimilarity</a></span><span class="op">(</span><span class="va">vegemat</span><span class="op">)</span></span>
<span></span>
<span><span class="va">clust1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/hclu_optics.html">hclu_optics</a></span><span class="op">(</span><span class="va">dissim</span><span class="op">)</span></span>
<span></span>
<span><span class="va">clust1</span></span></code></pre></div>
<pre><code><span><span class="co">## Clustering results for algorithm : hclu_optics </span></span>
<span><span class="co">##  - Number of sites:  715 </span></span>
<span><span class="co">## Clustering results:</span></span>
<span><span class="co">##  - Number of partitions:  1 </span></span>
<span><span class="co">##  - Number of clusters:  9</span></span></code></pre>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0" line-spacing="2">
<div id="ref-Baselga2012" class="csl-entry">
Baselga, A. (2012). The relationship between species replacement,
dissimilarity derived from nestedness, and nestedness. <em>Global
Ecology and Biogeography</em>, <em>21</em>(12), 1223–1232.
</div>
<div id="ref-Baselga2013" class="csl-entry">
Baselga, A. (2013). Separating the two components of abundance-based
dissimilarity: Balanced changes in abundance vs. Abundance gradients.
<em>Methods in Ecology and Evolution</em>, <em>4</em>(6), 552–557.
</div>
<div id="ref-Castro-Insua2018" class="csl-entry">
Castro-Insua, A., Gómez-Rodríguez, C., &amp; Baselga, A. (2018). <span class="nocase">Dissimilarity measures affected by richness differences
yield biased delimitations of biogeographic realms</span>. <em>Nature
Communications</em>, <em>9</em>(1), 9–11.
</div>
<div id="ref-Ficetola2017" class="csl-entry">
Ficetola, G. F., Mazel, F., &amp; Thuiller, W. (2017). <span class="nocase">Global determinants of zoogeographical boundaries</span>.
<em>Nature Ecology &amp; Evolution</em>, <em>1</em>, 0089.
</div>
<div id="ref-Hahsler2019" class="csl-entry">
Hahsler, M., Piekenbrock, M., &amp; Doran, D. (2019). Dbscan: Fast
density-based clustering with r. <em>Journal of Statistical
Software</em>, <em>91</em>(1), 1–30.
</div>
<div id="ref-Holt2013" class="csl-entry">
Holt, B. G., Lessard, J.-P., Borregaard, M. K., Fritz, S. A., Araújo, M.
B., Dimitrov, D., Fabre, P.-H., Graham, C. H., Graves, G. R., Jønsson,
K. a., Nogués-Bravo, D., Wang, Z., Whittaker, R. J., Fjeldså, J., &amp;
Rahbek, C. (2013). <span class="nocase">An update of Wallace’s
zoogeographic regions of the world</span>. <em>Science</em>,
<em>339</em>(6115), 74–78.
</div>
<div id="ref-Kaufman2009" class="csl-entry">
Kaufman, L., &amp; Rousseeuw, P. J. (2009). Finding groups in data: An
introduction to cluster analysis. In J. W. &amp; Sons. (Ed.),
<em>Finding groups in data: An introduction to cluster analysis.</em>
John Wiley &amp; Sons.
</div>
<div id="ref-Kreft2010" class="csl-entry">
Kreft, H., &amp; Jetz, W. (2010). <span class="nocase">A framework for
delineating biogeographical regions based on species
distributions</span>. <em>Journal of Biogeography</em>, <em>37</em>,
2029–2053.
</div>
<div id="ref-Leprieur2014" class="csl-entry">
Leprieur, F., &amp; Oikonomou, A. (2014). <span class="nocase">The need
for richness-independent measures of turnover when delineating
biogeographical regions</span>. <em>Journal of Biogeography</em>,
<em>41</em>, 417–420.
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Maxime Lenormand, Boris Leroy, Pierre Denelle.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.1.</p>
</div>

    </footer>
</div>





  </body>
</html>
