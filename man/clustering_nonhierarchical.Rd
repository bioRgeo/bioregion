% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/clustering_nonhierarchical.R
\name{clustering_nonhierarchical}
\alias{clustering_nonhierarchical}
\title{Non hierarchical clustering based on based on distances or beta-diversity}
\usage{
clustering_nonhierarchical(
  distances,
  index = names(distances)[3],
  method = "pam",
  n_clust = NULL,
  nclust_find_method = "firstSEmax",
  pam_nstart = 10,
  pam_variant = "faster",
  pam_cluster_only = FALSE,
  kmeans_iter_max = 10,
  kmeans_nstart = 10,
  dbscan_minPts = NULL,
  dbscan_eps = NULL,
  dbscan_plot = TRUE,
  optics_xi = 0.05
)
}
\arguments{
\item{distances}{the output object from \code{\link{similarity_to_distance}},
a \code{data.frame} with the first columns called "Site1" and "Site2", and
the other columns being the distance indices or a \code{dist} object}

\item{index}{a \code{character} string providing the name of the distance
index to use, corresponding to the column
name in \code{distances}. By default, the third column name of
 \code{distances} is used.}

\item{method}{a \code{character} vector specifying the methods to use. Available
methods: "pam", "kmeans", "dbscan", "optics", "mclust". Default is "pam".
Specifying "all" will run all clustering methods.}

\item{n_clust}{an \code{integer} specifying the requested number of cluster for pam
and kmeans.}

\item{nclust_find_method}{if n_clust is NULL, specificies the method to be
used to find the optimal number of clusters?  ##################################### TO BE IMPLEMENTED}

\item{pam_nstart}{an \code{integer} specifying the number of random “starts”
for the pam algorithm. By default, 1 (for the \code{"faster"} variant)}

\item{pam_variant}{a \code{character} string specifying the variant of pam
to use, by default "faster". See \link[cluster:pam]{cluster::pam()} for
more details}

\item{pam_cluster_only}{a \code{boolean} specifying if only the clustering
should be returned from the \link[cluster:pam]{cluster::pam()} function
(more efficient)}

\item{kmeans_iter_max}{an \code{integer} specifying the maximum number of
iterations for the kmeans method (see \link[stats:kmeans]{stats::kmeans()})}

\item{kmeans_nstart}{an \code{integer} specifying how many random sets of
\code{n_clust} should be selected as starting points for the kmeans analysis
(see \link[stats:kmeans]{stats::kmeans()})}

\item{dbscan_minPts}{a \code{numeric} value specifying the minPts argument
of \link[dbscan:dbscan]{dbscan::dbscan()}). By default, it is set to the
natural logarithm of the number of sites in \code{distances}.}

\item{dbscan_eps}{a \code{numeric} value specifying the eps argument
of \link[dbscan:dbscan]{dbscan::dbscan()}). The value of eps depends on the
minPts argument, and should be chosen by identifying a knee in the k-nearest
neighbour distance plot. By default the function will try to automatically
find a knee, but the result is uncertain, and so the user should inspect the
graph and modify \code{dbscan_eps} accordingly.}

\item{dbscan_plot}{a \code{boolean} indicating if the  k-nearest
neighbour distance plot should be plotted.}

\item{optics_xi}{a \code{numeric} value specifying}
}
\value{
to fill
}
\description{
This function includes several algorithms to perform non hierarchical
clustering on the basis of distances: partioning around medoids / k-medoids,
k-means, dbscan, optics, and model-based clustering based on Gaussian
 mixture models.
}
\details{
\itemize{
 \item{\bold{Partitioning around medoids}: This method partitions data into
 the chosen number of cluster on the basis of the input distance matrix.
 It is more robust than k-means because it minimizes the sum of distances
 between cluster centres and points assigned to the cluster -
 whereas the k-means approach minimizes the sum of squared euclidean
 distances (thus k-means cannot be applied directly on the input distance
 matrix if the distances are not euclidean).}
 \item{\bold{k-means}: This method partitions the data into k groups
 such that that the sum of squares of euclidean distances from points to the
 assigned cluster centres is minimized. k-means cannot be applied directly
 on dissimilarity/beta-diversity metrics, because these distances are not
 euclidean. Therefore, it requires first to transform the distance matrix
 with a Principal Coordinate Analysis (using the function
 \link[ape:pcoa]{ape::pcoa()}), and then applying k-means on the coordinates
 of points in the PCoA.}
 \item{\bold{dbscan}: The dbscan (Density-based spatial clustering of
 applications with noise) clustering algorithm clusters points on the basis
 of the density of neighbours around each data points. It necessitates two
 main arguments, minPts, which stands for the minimum number of points to
 identify a core, and eps, which is the radius to find neighbours.
 minPts and eps should be defined by the user, which is not straightforward.
 We recommend reading the helpo in \link[dbscan:dbscan]{dbscan::dbscan()})
 to learn how to set these arguments, as well as the paper
 \insertCite{Hahsler2019}{bioRgeo}. Note that clusters with a value of 0
 are points which were deemed as noise by the algorithm.
  }
 \item{\bold{optics}: The optics (Ordering points to identify the clustering
 structure) is a semi-hierarchical clustering algorithm which orders the
 points in the dataset such that points which are closest become neighbours,
 and calculates a reachability distance for each point. Then, clusters
 can be extracted in a hierarchical manner from this reachability distance,
 by identifying clusters depending on changes in the relative cluster
 density. The reachability plot should be explored to understand
 the clusters and their hierarchical nature, by running plot on the output
 of the function: \code{plot(object$outputs$clustering_algorithms$optics)}.
 We recommend reading \insertCite{Hahsler2019}{bioRgeo} to grasp the
 algorithm, how it works, and what the clusters mean.}
 \item{\bold{mclust}: Model-based clustering based on Gaussian mixture
 models. This method is normally designed to be applied on the raw dataset,
 i.e. not directly on the distance matrix. However, we have found that it
 provides insightful results when applied on a distance matrix. Because this
 is an unusual application, we recommend users to be careful with the
 interpretation. See \insertCite{Scrucca2016}{bioRgeo} for more information
 on this method.}

 }
}
\examples{
comat <- matrix(sample(0:1000, size = 500, replace = TRUE, prob = 1/1:1001), 20, 25)
rownames(comat) <- paste0("Site",1:20)
colnames(comat) <- paste0("Species",1:25)

simil <- similarity(comat, metric = "all")
distances <- similarity_to_distance(simil)

# User-defined number of clusters
# clust1 <- clustering_nonhierarchical(distances,
#                                    n_clust = 5,
#                                     index = "Simpson")

#  clust2 <- clustering_nonhierarchical(distances,
#                                     method = c("pam", "kmeans", "dbscan",
#                                                 "optics", "mclust"),
#                                     n_clust = 5,
#                                     index = "Simpson",
#                                     dbscan_minPts = 5,
#                                     dbscan_eps = 0.06)
}
